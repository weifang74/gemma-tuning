{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 0: Install Tunix and dependencies\n",
    "\n",
    "Installs **Tunix** (Google’s JAX/TPU-first training and serving utilities) with the `prod` extras.  \n",
    "This notebook assumes a Kaggle TPU runtime; installing inside the notebook guarantees the exact version (`0.1.3`) used when the notebook was authored.\n",
    "\n",
    "**Notes**\n",
    "- If you see dependency conflicts, restart the kernel after installation.\n",
    "- Pinning the version helps reproducibility across Kaggle sessions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-21T02:23:33.425968Z",
     "iopub.status.busy": "2025-12-21T02:23:33.425318Z",
     "iopub.status.idle": "2025-12-21T02:23:46.545550Z",
     "shell.execute_reply": "2025-12-21T02:23:46.544477Z",
     "shell.execute_reply.started": "2025-12-21T02:23:33.425945Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-tunix==0.1.3 in /usr/local/lib/python3.12/site-packages (from google-tunix[prod]==0.1.3) (0.1.3)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.4.1)\n",
      "Requirement already satisfied: flax>=0.11.1 in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.12.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2025.10.0)\n",
      "Requirement already satisfied: gcsfs in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2025.10.0)\n",
      "Requirement already satisfied: grain in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.2.14)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.36.0)\n",
      "Requirement already satisfied: jaxtyping in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.1.6)\n",
      "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.3.13)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.63.0b1)\n",
      "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.4.0.dev4)\n",
      "Requirement already satisfied: pylatexenc in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.0a33)\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.2.1)\n",
      "Requirement already satisfied: qwix in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.2)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.2.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.14.0)\n",
      "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.6.4)\n",
      "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.9.9)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.67.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.57.1)\n",
      "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.9)\n",
      "Requirement already satisfied: jax!=0.7.2,>=0.6.0 in /usr/local/lib/python3.12/site-packages (from jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (0.8.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.12/site-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.3.4)\n",
      "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/site-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.1.2)\n",
      "Requirement already satisfied: optax in /usr/local/lib/python3.12/site-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.2.6)\n",
      "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.12/site-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.11.27)\n",
      "Requirement already satisfied: tensorstore in /usr/local/lib/python3.12/site-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.78)\n",
      "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.12/site-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (14.2.0)\n",
      "Requirement already satisfied: typing_extensions>=4.2 in /usr/local/lib/python3.12/site-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.15.0)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.12/site-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (6.0.3)\n",
      "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.12/site-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.10)\n",
      "Requirement already satisfied: jaxlib<=0.8.0,>=0.8.0 in /usr/local/lib/python3.12/site-packages (from jax!=0.7.2,>=0.6.0->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (0.8.0)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/site-packages (from jax!=0.7.2,>=0.6.0->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (0.5.3)\n",
      "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/site-packages (from jax!=0.7.2,>=0.6.0->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.13 in /usr/local/lib/python3.12/site-packages (from jax!=0.7.2,>=0.6.0->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (1.16.3)\n",
      "Collecting libtpu==0.0.24.* (from jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3)\n",
      "  Downloading libtpu-0.0.24-cp312-cp312-manylinux_2_31_x86_64.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/site-packages (from jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (2.32.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/site-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/site-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/site-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.4.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/site-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/site-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/site-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/site-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.70.18)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/site-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (25.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/site-packages (from huggingface_hub->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.2.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/site-packages (from gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.13.2)\n",
      "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.12/site-packages (from gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (5.2.1)\n",
      "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.12/site-packages (from gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.42.1)\n",
      "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.12/site-packages (from gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.2.2)\n",
      "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.12/site-packages (from gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.5.0)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/site-packages (from grain->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.3.1)\n",
      "Requirement already satisfied: array-record>=0.8.1 in /usr/local/lib/python3.12/site-packages (from grain->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.8.2)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/site-packages (from grain->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.1.2)\n",
      "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.12/site-packages (from grain->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.13.0)\n",
      "Requirement already satisfied: protobuf>=5.28.3 in /usr/local/lib/python3.12/site-packages (from grain->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (6.33.0)\n",
      "Requirement already satisfied: wadler-lindig>=0.1.3 in /usr/local/lib/python3.12/site-packages (from jaxtyping->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/site-packages (from jinja2->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.0.3)\n",
      "Requirement already satisfied: llvmlite<0.47,>=0.46.0dev0 in /usr/local/lib/python3.12/site-packages (from numba->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.46.0b1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/site-packages (from sympy->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.3.0)\n",
      "Requirement already satisfied: dm-tree in /usr/local/lib/python3.12/site-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.9)\n",
      "Requirement already satisfied: immutabledict in /usr/local/lib/python3.12/site-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.2.2)\n",
      "Requirement already satisfied: promise in /usr/local/lib/python3.12/site-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.3)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/site-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (7.1.3)\n",
      "Requirement already satisfied: simple_parsing in /usr/local/lib/python3.12/site-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.7)\n",
      "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.12/site-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.17.2)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/site-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.2.0)\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.12/site-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.10.2)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/site-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.0.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/site-packages (from transformers->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/site-packages (from transformers->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/site-packages (from transformers->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.7.0rc0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.22.0)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.8.1)\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (6.5.2)\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.23.0)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/site-packages (from google-auth>=1.2->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (6.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/site-packages (from google-auth>=1.2->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/site-packages (from google-auth>=1.2->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.9.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/site-packages (from httpx<1.0.0->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/site-packages (from httpx<1.0.0->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/site-packages (from httpx<1.0.0->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/site-packages (from httpx<1.0.0->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (2.5.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/site-packages (from rich>=11.1->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/site-packages (from rich>=11.1->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.19.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.12/site-packages (from google-auth-oauthlib->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.0.0)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=2.27.0 in /usr/local/lib/python3.12/site-packages (from google-cloud-storage->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.28.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.2 in /usr/local/lib/python3.12/site-packages (from google-cloud-storage->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.5.0)\n",
      "Requirement already satisfied: google-resumable-media<3.0.0,>=2.7.2 in /usr/local/lib/python3.12/site-packages (from google-cloud-storage->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.7.2)\n",
      "Requirement already satisfied: google-crc32c<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/site-packages (from google-cloud-storage->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.7.1)\n",
      "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.12/site-packages (from optax->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.91)\n",
      "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/site-packages (from orbax-checkpoint->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.6.0)\n",
      "Requirement already satisfied: aiofiles in /usr/local/lib/python3.12/site-packages (from orbax-checkpoint->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (25.1.0)\n",
      "Requirement already satisfied: humanize in /usr/local/lib/python3.12/site-packages (from orbax-checkpoint->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.14.0)\n",
      "Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.12/site-packages (from orbax-checkpoint->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.20.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/site-packages (from pandas->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/site-packages (from pandas->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/site-packages (from pandas->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2025.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.12/site-packages (from promise->tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.17.0)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.12/site-packages (from simple_parsing->tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.17.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /usr/local/lib/python3.12/site-packages (from tensorflow-metadata->tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.71.0)\n",
      "Requirement already satisfied: toolz>=1.0.0 in /usr/local/lib/python3.12/site-packages (from chex>=0.1.87->optax->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.1.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/site-packages (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.26.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.3.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/site-packages (from anyio->httpx<1.0.0->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.3.1)\n",
      "Downloading libtpu-0.0.24-cp312-cp312-manylinux_2_31_x86_64.whl (156.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.4/156.4 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: libtpu\n",
      "  Attempting uninstall: libtpu\n",
      "    Found existing installation: libtpu 0.0.17\n",
      "    Uninstalling libtpu-0.0.17:\n",
      "      Successfully uninstalled libtpu-0.0.17\n",
      "Successfully installed libtpu-0.0.24\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install \"google-tunix[prod]==0.1.3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: TPU/JAX runtime sanity checks + environment flags\n",
    "\n",
    "1. Imports JAX and prints a quick **device inventory** (backend, device kind, device list).  \n",
    "2. Warns if you are not on TPU (important because Gemma 3 training is intended to run on TPU in this notebook).  \n",
    "3. Sets several environment variables and JAX configs:\n",
    "   - `XLA_FLAGS` and `LIBTPU_INIT_ARGS`: performance and async collective behavior.\n",
    "   - `JAX_COMPILATION_CACHE_DIR`: speeds up repeated compiles.\n",
    "   - `jax_enable_x64=False`: keeps computation in 32-bit (typically BF16/FP32 mix) for speed/memory.\n",
    "   - `jax_default_matmul_precision='high'`: improves numerical stability for matmuls.\n",
    "\n",
    "**Pitfall**\n",
    "- If `jax.default_backend()` is not `tpu`, training will be extremely slow and results will not match the intended setup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T02:23:46.546550Z",
     "iopub.status.busy": "2025-12-21T02:23:46.546365Z",
     "iopub.status.idle": "2025-12-21T02:23:47.762861Z",
     "shell.execute_reply": "2025-12-21T02:23:47.761625Z",
     "shell.execute_reply.started": "2025-12-21T02:23:46.546531Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "googleapis-common-protos     1.71.0\n",
      "proto-plus                   1.26.1\n",
      "protobuf                     6.33.0\n"
     ]
    }
   ],
   "source": [
    "!pip list |grep proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T02:23:47.763735Z",
     "iopub.status.busy": "2025-12-21T02:23:47.763548Z",
     "iopub.status.idle": "2025-12-21T02:24:12.962920Z",
     "shell.execute_reply": "2025-12-21T02:24:12.961865Z",
     "shell.execute_reply.started": "2025-12-21T02:23:47.763717Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/tensorboard/default.py:30: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "/usr/local/lib/python3.12/site-packages/jax/_src/cloud_tpu_init.py:93: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
      "  warnings.warn(\n",
      "usage: tensorboard [-h] [--helpfull] {serve} ...\n",
      "tensorboard: error: argument {serve}: invalid choice: 'outputs_sft_full/tensorboard' (choose from serve)\n"
     ]
    }
   ],
   "source": [
    "!tensorboard outputs_sft_full/tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T02:24:12.964013Z",
     "iopub.status.busy": "2025-12-21T02:24:12.963830Z",
     "iopub.status.idle": "2025-12-21T02:24:22.252599Z",
     "shell.execute_reply": "2025-12-21T02:24:22.251519Z",
     "shell.execute_reply.started": "2025-12-21T02:24:12.963995Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX version: 0.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before InitGoogle() is written to STDERR\n",
      "E0000 00:00:1766283854.283710      12 common_lib.cc:648] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n",
      "=== Source Location Trace: === \n",
      "learning/45eac/tfrc/runtime/common_lib.cc:238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 8\n",
      "Device kind: TPU v5 lite\n",
      "JAX backend: tpu\n",
      "\n",
      "Devices:\n",
      "  [0] TPU_0(process=0,(0,0,0,0))\n",
      "  [1] TPU_1(process=0,(1,0,0,0))\n",
      "  [2] TPU_2(process=0,(0,1,0,0))\n",
      "  [3] TPU_3(process=0,(1,1,0,0))\n",
      "  [4] TPU_4(process=0,(0,2,0,0))\n",
      "  [5] TPU_5(process=0,(1,2,0,0))\n",
      "  [6] TPU_6(process=0,(0,3,0,0))\n",
      "  [7] TPU_7(process=0,(1,3,0,0))\n",
      "============================================================\n",
      "\n",
      "✓ TPU backend confirmed\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import os\n",
    "import warnings; \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Number of devices: {len(jax.devices())}\")\n",
    "print(f\"Device kind: {jax.devices()[0].device_kind}\")\n",
    "print(f\"JAX backend: {jax.default_backend()}\")\n",
    "print(f\"\\nDevices:\")\n",
    "for i, device in enumerate(jax.devices()):\n",
    "    print(f\"  [{i}] {device}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if jax.default_backend() != 'tpu':\n",
    "    print(\"\\n⚠️  WARNING: Not running on TPU!\")\n",
    "    print(f\"   Current backend: {jax.default_backend()}\")\n",
    "    print(\"   Make sure you've selected TPU runtime in Kaggle\")\n",
    "else:\n",
    "    print(\"\\n✓ TPU backend confirmed\")\n",
    "\n",
    "\n",
    "os.environ['XLA_FLAGS'] = (\n",
    "    '--xla_gpu_enable_triton_softmax_fusion=true '\n",
    "    '--xla_gpu_triton_gemm_any=True '\n",
    "    '--xla_gpu_enable_async_collectives=true'\n",
    ")\n",
    "os.environ['JAX_COMPILATION_CACHE_DIR'] = '/tmp/jax_cache'\n",
    "os.environ['LIBTPU_INIT_ARGS'] = '--xla_enable_async_all_gather=true'\n",
    "\n",
    "jax.config.update('jax_enable_x64', False)  # Use 32-bit for speed\n",
    "jax.config.update('jax_default_matmul_precision', 'high')  # BF16 matmuls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Experiment configuration (model, batching, training hyperparameters, output paths)\n",
    "\n",
    "Defines the main knobs for fine-tuning:\n",
    "\n",
    "- **Model handle** (`KAGGLE_MODEL_HANDLE`): points to Gemma 3 weights hosted on Kaggle.\n",
    "- **Sequence length** (`MAX_SEQ_LENGTH`): max tokens per example; impacts memory and speed.\n",
    "- **TPU mesh** (`MESH_SHAPE`): logical device mesh for sharding (FSDP axis and tensor-parallel axis).\n",
    "- **Micro-batch size** + **gradient accumulation**: together determine the **effective global batch size**.\n",
    "- **Optimizer hyperparams**: learning rate, warmup, weight decay, grad clipping, epochs/steps.\n",
    "- **Checkpoint/TensorBoard dirs** and logging cadence.\n",
    "\n",
    "The printed “Global Batch Size” helps confirm your true effective batch:\n",
    "`micro_batch * num_devices * grad_accumulation`.\n",
    "\n",
    "**Note**\n",
    "- The `...` line in this cell is a placeholder in the original notebook source. If you run the notebook as-is, ensure all required constants (e.g., Adam betas/epsilon if referenced later) are defined somewhere.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T02:49:16.419836Z",
     "iopub.status.busy": "2025-12-21T02:49:16.419602Z",
     "iopub.status.idle": "2025-12-21T02:49:16.424941Z",
     "shell.execute_reply": "2025-12-21T02:49:16.424066Z",
     "shell.execute_reply.started": "2025-12-21T02:49:16.419819Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Batch Size: 64\n",
      "Total Training Steps: 3000\n",
      "✓ Configuration loaded\n"
     ]
    }
   ],
   "source": [
    "KAGGLE_MODEL_HANDLE = \"google/gemma-3/transformers/gemma-3-1b-it\"\n",
    "\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "MESH_SHAPE = (8, 1) \n",
    "TRAIN_MICRO_BATCH_SIZE = 2 \n",
    "\n",
    "GRADIENT_ACCUMULATION_STEPS = 4 \n",
    "\n",
    "LEARNING_RATE = 2e-5 \n",
    "WARMUP_STEPS = 50    \n",
    "NUM_EPOCHS = 3       \n",
    "\n",
    "\n",
    "MAX_STEPS = 200 * NUM_EPOCHS \n",
    "\n",
    "\n",
    "ADAM_BETA1 = 0.9\n",
    "\n",
    "ADAM_BETA2 = 0.999 \n",
    "\n",
    "ADAM_EPSILON = 1e-8\n",
    "\n",
    "\n",
    "WEIGHT_DECAY = 0.01 \n",
    "MAX_GRAD_NORM = 1.0\n",
    "\n",
    "print(f\"Global Batch Size: {TRAIN_MICRO_BATCH_SIZE * 8 * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"Total Training Steps: {MAX_STEPS}\")\n",
    "\n",
    "\n",
    "CHECKPOINT_DIR = \"/kaggle/working/outputs_sft_full/checkpoints\"\n",
    "TENSORBOARD_DIR = \"/kaggle/working/outputs_sft_full/tensorboard\"\n",
    "SAVE_INTERVAL_STEPS = 100\n",
    "EVAL_INTERVAL_STEPS = 50\n",
    "LOG_INTERVAL_STEPS = 10\n",
    "\n",
    "print(\"✓ Configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Download Gemma 3 from Kaggle and create a TPU device mesh\n",
    "\n",
    "- Uses `kagglehub.model_download()` to fetch the model assets locally.\n",
    "- Builds a JAX mesh (`jax.make_mesh`) with axes `('fsdp', 'tp')` using `MESH_SHAPE`.\n",
    "\n",
    "This mesh is later used to:\n",
    "- **Shard parameters** across devices (FSDP-style parameter sharding).\n",
    "- Optionally use a tensor-parallel axis (depending on model/implementation).\n",
    "\n",
    "**Why this matters**\n",
    "Without a mesh context, the model can silently remain on CPU, making training incorrect/slow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T02:49:21.499658Z",
     "iopub.status.busy": "2025-12-21T02:49:21.499408Z",
     "iopub.status.idle": "2025-12-21T02:49:21.925357Z",
     "shell.execute_reply": "2025-12-21T02:49:21.924330Z",
     "shell.execute_reply.started": "2025-12-21T02:49:21.499640Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model handle: google/gemma-3/transformers/gemma-3-1b-it\n",
      "✓ Model downloaded to: /kaggle/input/gemma-3/transformers/gemma-3-1b-it/1\n",
      "\n",
      "Creating TPU mesh with shape (8, 1)...\n",
      "✓ TPU Mesh created successfully\n",
      "  Mesh shape: OrderedDict({'fsdp': 8, 'tp': 1})\n",
      "  Mesh axis names: ('fsdp', 'tp')\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "from tunix.models.gemma3 import model as gemma_lib\n",
    "from tunix.models.gemma3 import params_safetensors as params_safetensors_lib\n",
    "from tunix.generate import tokenizer_adapter as tokenizer_lib\n",
    "\n",
    "print(f\"Model handle: {KAGGLE_MODEL_HANDLE}\")\n",
    "\n",
    "local_model_path = kagglehub.model_download(KAGGLE_MODEL_HANDLE)\n",
    "print(f\"✓ Model downloaded to: {local_model_path}\")\n",
    "\n",
    "print(f\"\\nCreating TPU mesh with shape {MESH_SHAPE}...\")\n",
    "mesh = jax.make_mesh(MESH_SHAPE, ('fsdp', 'tp'))\n",
    "print(f\"✓ TPU Mesh created successfully\")\n",
    "print(f\"  Mesh shape: {mesh.shape}\")\n",
    "print(f\"  Mesh axis names: {mesh.axis_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Load model weights (.safetensors) and tokenizer\n",
    "\n",
    "- Creates a Gemma 3 1B model config.\n",
    "- Loads model parameters from the downloaded `.safetensors` files into a JAX/Flax model, sharded according to the TPU mesh.\n",
    "- Loads the SentencePiece tokenizer (`tokenizer.model`) matching the base checkpoint.\n",
    "\n",
    "**Key idea**\n",
    "Tokenizer and model weights must match; mixing tokenizers across checkpoints can corrupt training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T02:49:25.814105Z",
     "iopub.status.busy": "2025-12-21T02:49:25.813844Z",
     "iopub.status.idle": "2025-12-21T02:49:27.205929Z",
     "shell.execute_reply": "2025-12-21T02:49:27.204905Z",
     "shell.execute_reply.started": "2025-12-21T02:49:25.814084Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded successfully\n",
      "✓ Tokenizer loaded successfully\n"
     ]
    }
   ],
   "source": [
    "model_config = gemma_lib.ModelConfig.gemma3_1b()\n",
    "\n",
    "gemma3_model = params_safetensors_lib.create_model_from_safe_tensors(\n",
    "    local_model_path,  # Directory containing .safetensors files\n",
    "    model_config,\n",
    "    mesh,\n",
    ")\n",
    "print(\"✓ Model loaded successfully\")\n",
    "\n",
    "\n",
    "tokenizer = tokenizer_lib.Tokenizer(\n",
    "    tokenizer_path=f\"{local_model_path}/tokenizer.model\"\n",
    ")\n",
    "print(\"✓ Tokenizer loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Force model parameter sharding onto TPU and verify placement\n",
    "\n",
    "- Uses `flax.nnx` utilities to:\n",
    "  - extract model state (`nnx.state`)\n",
    "  - compute partition specs (`nnx.get_partition_spec`)\n",
    "  - apply sharding constraints (`jax.lax.with_sharding_constraint`)\n",
    "  - update the model with the sharded state (`nnx.update`)\n",
    "- “Materializes” shapes to force device placement.\n",
    "- Then inspects a sample parameter to confirm it resides on TPU devices.\n",
    "\n",
    "**Why this exists**\n",
    "In JAX it is possible to construct objects on host/CPU and only later place them on device. This explicit sharding/verification prevents a common failure mode: “training runs but on CPU”.\n",
    "\n",
    "**Note about `...`**\n",
    "The `...` line is a placeholder from the original notebook and is not executable Python. If this notebook errors at runtime, remove/replace those placeholders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T02:49:28.786813Z",
     "iopub.status.busy": "2025-12-21T02:49:28.786531Z",
     "iopub.status.idle": "2025-12-21T02:49:29.033653Z",
     "shell.execute_reply": "2025-12-21T02:49:29.032603Z",
     "shell.execute_reply.started": "2025-12-21T02:49:28.786794Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sharding model across TPU devices...\n",
      "\n",
      "✓ Model ready for full fine-tuning\n",
      "Total parameters: 999,885,952\n",
      "Trainable parameters: 999,885,952\n",
      "Number of parameters: 314\n",
      "Sample param shape: (262144, 1152)\n",
      "Sample param dtype: bfloat16\n",
      "Sample param devices: [TpuDevice(id=7, process_index=0, coords=(1,3,0), core_on_chip=0), TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=6, process_index=0, coords=(0,3,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(1,2,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=4, process_index=0, coords=(0,2,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)]\n",
      "Device kind: TPU v5 lite\n",
      "✓✓✓ SUCCESS: Model parameters are on TPU!\n",
      "✓✓✓ Confirmed: TPU v5 lite detected\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import flax.nnx as nnx\n",
    "\n",
    "\n",
    "model_input = gemma3_model.get_model_input()\n",
    "\n",
    "print(\"\\nSharding model across TPU devices...\")\n",
    "with mesh:\n",
    "    state = nnx.state(gemma3_model)\n",
    "    pspecs = nnx.get_partition_spec(state)\n",
    "    sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
    "    nnx.update(gemma3_model, sharded_state)\n",
    "    \n",
    "    # Force materialization on TPU\n",
    "    _ = jax.tree_util.tree_map(lambda x: x.shape if hasattr(x, 'shape') else x, state)\n",
    "    \n",
    "\n",
    "\n",
    "total_params = sum(p.size for p in jax.tree_util.tree_leaves(nnx.state(gemma3_model)))\n",
    "\n",
    "print(f\"\\n✓ Model ready for full fine-tuning\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {total_params:,}\")\n",
    "\n",
    "\n",
    "all_params = nnx.state(gemma3_model)\n",
    "param_leaves = jax.tree_util.tree_leaves(all_params)\n",
    "print(f\"Number of parameters: {len(param_leaves)}\")\n",
    "\n",
    "if len(param_leaves) > 0:\n",
    "    sample = param_leaves[0]\n",
    "    print(f\"Sample param shape: {sample.shape}\")\n",
    "    print(f\"Sample param dtype: {sample.dtype}\")\n",
    "    \n",
    "    # Check device placement\n",
    "    if hasattr(sample, 'devices'):\n",
    "        devices_set = sample.devices()\n",
    "        print(f\"Sample param devices: {list(devices_set)}\")\n",
    "        if len(devices_set) > 0:\n",
    "            dev = list(devices_set)[0]\n",
    "            device_kind = dev.device_kind\n",
    "            print(f\"Device kind: {device_kind}\")\n",
    "            if 'tpu' in device_kind.lower():\n",
    "                print(\"✓✓✓ SUCCESS: Model parameters are on TPU!\")\n",
    "                print(f\"✓✓✓ Confirmed: {device_kind} detected\")\n",
    "            else:\n",
    "                print(f\"❌❌❌ ERROR: Model parameters are on {device_kind}, NOT TPU!\")\n",
    "                print(\"Training will run on CPU and produce wrong results!\")\n",
    "    else:\n",
    "        print(\"⚠️  Cannot determine device placement\")\n",
    "else:\n",
    "    print(\"❌ NO parameters found!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Build an inference sampler (generation) + prompt constructor\n",
    "\n",
    "- Configures the KV cache (`CacheConfig`) for autoregressive generation.\n",
    "- Instantiates `sampler_lib.Sampler` with the model and tokenizer.\n",
    "- Defines `generate_inference_prompt(question)` which formats the input exactly like training:\n",
    "  - `<start_of_turn>user` + system instructions + question\n",
    "  - `<start_of_turn>model` + opens `<reasoning>` tag (the model is expected to continue)\n",
    "\n",
    "**Why it matters**\n",
    "Evaluation should mirror training formatting to get an apples-to-apples baseline and post-training comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T02:49:45.139953Z",
     "iopub.status.busy": "2025-12-21T02:49:45.139718Z",
     "iopub.status.idle": "2025-12-21T02:49:45.536232Z",
     "shell.execute_reply": "2025-12-21T02:49:45.535129Z",
     "shell.execute_reply.started": "2025-12-21T02:49:45.139936Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tunix.generate import sampler as sampler_lib\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "cache_config = sampler_lib.CacheConfig(\n",
    "    cache_size=MAX_SEQ_LENGTH + 512,\n",
    "    num_layers=model_config.num_layers,\n",
    "    num_kv_heads=model_config.num_kv_heads,\n",
    "    head_dim=model_config.head_dim,\n",
    ")\n",
    "\n",
    "\n",
    "generation_sampler = sampler_lib.Sampler(\n",
    "    transformer=gemma3_model,\n",
    "    tokenizer=tokenizer,\n",
    "    cache_config=cache_config,\n",
    ")\n",
    "\n",
    "\n",
    "def generate_inference_prompt(question):\n",
    "    # Match the training exactly: Same System Prompt, No One-Shot needed anymore.\n",
    "    text = f\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n\\n{question}<end_of_turn>\\n\"\n",
    "    text += f\"<start_of_turn>model\\n<reasoning>\\n\" \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Define the strict instruction format and templates\n",
    "\n",
    "Sets up:\n",
    "- XML-style tags used in training/eval:\n",
    "  - `<reasoning>...</reasoning>`\n",
    "  - `<answer>...</answer>`\n",
    "- A **SYSTEM_PROMPT** that forces the model to follow the schema.\n",
    "- Prompt templates showing how a full supervised example is constructed.\n",
    "\n",
    "**Goal**\n",
    "This is “schema SFT”: you teach the model not just to solve problems, but to consistently produce machine-parseable outputs.\n",
    "\n",
    "**Note**\n",
    "`PROMPT_TEMPLATE` contains a `...` placeholder in the notebook. Replace it with a concrete template if you intend to use it directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T02:49:49.259880Z",
     "iopub.status.busy": "2025-12-21T02:49:49.259586Z",
     "iopub.status.idle": "2025-12-21T02:49:49.263701Z",
     "shell.execute_reply": "2025-12-21T02:49:49.262619Z",
     "shell.execute_reply.started": "2025-12-21T02:49:49.259861Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from datasets import load_dataset\n",
    "reasoning_start = \"<reasoning>\"\n",
    "reasoning_end = \"</reasoning>\"\n",
    "solution_start = \"<answer>\"\n",
    "solution_end = \"</answer>\"\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "\"\"\"You are a helpful assistant.\n",
    "Please solve the problem and STRICTLY follow this output format:\n",
    "<reasoning>\n",
    "[Your step-by-step thinking process here]\n",
    "</reasoning>\n",
    "<answer>\n",
    "[Final answer here]\n",
    "</answer>\n",
    "\n",
    "Here is an example:\n",
    "User: The present age of A is twice that of B. After 10 years, A will be 1.5 times B. What is A's present age?\n",
    "Model:\n",
    "<reasoning>\n",
    "Let A's present age be 'x' and B's present age be 'y'.\n",
    "We are given that the present age of A is twice that of B, so A = 2 * B.\n",
    "After 10 years, A will be x + 10 and B will be y + 10.\n",
    "We are given that after 10 years, A will be 1.5 times B, so x + 10 = 1.5 * (y + 10).\n",
    "Now we have two equations:\n",
    "1) x = 2 * y\n",
    "2) x + 10 = 1.5(y + 10)\n",
    "Substitute the first equation into the second equation:\n",
    "2 * y + 10 = 1.5(y + 10)\n",
    "2y + 10 = 1.5y + 15\n",
    "0.5y = 5\n",
    "y = 10\n",
    "Now, substitute y = 10 into x = 2 * y:\n",
    "x = 2 * 10\n",
    "x = 20\n",
    "So, A's present age is 20.\n",
    "</reasoning>\n",
    "<answer>\n",
    "20\n",
    "</answer\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"<start_of_turn>user\n",
    "{system_instruction}\\n\\n\n",
    "\n",
    "{question}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "FULL_TEMPLATE = \"\"\"<start_of_turn>user\n",
    "{system_prompt}\n",
    "\n",
    "{question}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\n",
    "{reasoning_start}\n",
    "{reasoning}\n",
    "{reasoning_end}\n",
    "\n",
    "{solution_start}\n",
    "{answer}\n",
    "{solution_end}<end_of_turn>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Load evaluation questions + define answer extraction and scoring\n",
    "\n",
    "- Attempts to load a CSV of questions and gold answers (`updated_200_math_questions.csv`).\n",
    "  - Falls back to a hard-coded list if the CSV is missing.\n",
    "- Defines utilities to:\n",
    "  - extract a final answer from the model output (prefer `<answer>`, then GSM8K `####`, then last numeric token)\n",
    "  - normalize answers (strip commas/currency, normalize whitespace/case)\n",
    "  - compare predictions to gold answers, including handling cases like `\"x or y\"`.\n",
    "\n",
    "**Why this is important**\n",
    "LLM outputs are messy. Robust evaluation requires:\n",
    "1) deterministic parsing rules, and  \n",
    "2) normalization to avoid false negatives from formatting differences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T02:50:06.386675Z",
     "iopub.status.busy": "2025-12-21T02:50:06.386407Z",
     "iopub.status.idle": "2025-12-21T02:50:06.401296Z",
     "shell.execute_reply": "2025-12-21T02:50:06.400289Z",
     "shell.execute_reply.started": "2025-12-21T02:50:06.386655Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: CSV: /kaggle/input/maths-sft-training-dataset/updated_200_math_questions.csv (200 rows)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load questions\n",
    "# -----------------------------\n",
    "# Option A: Evaluate from CSV (recommended for your 200 questions)\n",
    "CSV_PATH = \"/kaggle/input/maths-sft-training-dataset/updated_200_math_questions.csv\"   # adjust if needed\n",
    "\n",
    "try:\n",
    "    dfq = pd.read_csv(CSV_PATH)\n",
    "    questions = dfq[\"question\"].tolist()\n",
    "    golds = dfq[\"gold_answer\"].astype(str).tolist()\n",
    "    source = f\"CSV: {CSV_PATH} ({len(dfq)} rows)\"\n",
    "except Exception as e:\n",
    "    # Option B: fallback to your manual list\n",
    "    test_questions = [\n",
    "        \"What is the square root of 144?\",\n",
    "        \"If a shirt costs $25 and is on sale for 20% off, what is the sale price?\",\n",
    "        \"A train travels 60 miles in 45 minutes. What is its speed in miles per hour?\",\n",
    "        \"What is 15% of 200?\",\n",
    "        \"A product is marked up by 25% and then discounted by 20%. The final price is ₹960. What was the original price?\",\n",
    "        \"A car travels at 60 km/h for 30 minutes, stops for 10 minutes, then travels at 40 km/h for another 30 minutes. What is the car’s average speed for the entire journey?\",\n",
    "        \"What is ⅔ of ¾ of 120, minus 25% of the result?\",\n",
    "        \"The ratio of apples to oranges in a basket is 3:5. If 16 oranges are removed and the new ratio becomes 3:1,how many apples were originally in the basket?\",\n",
    "        \"A pipe fills a tank in 40 minutes, while another pipe empties the same tank in 60 minutes. If both pipes are opened together, how long will it take to fill the tank?\",\n",
    "        \"A number increases by 10% and then decreases by 10%. Is the final number greater than, less than, or equal to the original? Explain why.\",\n",
    "    ]\n",
    "    questions = test_questions\n",
    "    golds = [None] * len(questions)  # no golds in this path\n",
    "    source = f\"Manual list ({len(questions)} questions)\"\n",
    "    print(\"CSV load failed, using manual list. Error:\", e)\n",
    "\n",
    "print(\"Evaluating:\", source)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Helpers: normalize + extract answers\n",
    "# -----------------------------\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = str(s).strip().lower()\n",
    "    # normalize unicode fractions (⅔ etc.) if they appear in answers (rare)\n",
    "    s = (s.replace(\"½\", \"1/2\")\n",
    "           .replace(\"⅓\", \"1/3\").replace(\"⅔\", \"2/3\")\n",
    "           .replace(\"¼\", \"1/4\").replace(\"¾\", \"3/4\")\n",
    "           .replace(\"⅕\", \"1/5\").replace(\"⅖\", \"2/5\").replace(\"⅗\", \"3/5\").replace(\"⅘\", \"4/5\")\n",
    "           .replace(\"⅙\", \"1/6\").replace(\"⅚\", \"5/6\")\n",
    "           .replace(\"⅛\", \"1/8\").replace(\"⅜\", \"3/8\").replace(\"⅝\", \"5/8\").replace(\"⅞\", \"7/8\"))\n",
    "    # remove spaces around common separators\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    # remove currency symbols but keep numbers/units\n",
    "    s = s.replace(\"₹\", \"\").replace(\"$\", \"\")\n",
    "    # remove commas in numbers: 62,500 -> 62500\n",
    "    s = re.sub(r\"(?<=\\d),(?=\\d)\", \"\", s)\n",
    "    # trim punctuation at ends\n",
    "    s = s.strip(\" .,:;!?\\n\\t\")\n",
    "    return s\n",
    "\n",
    "def extract_final_from_response(response: str) -> str:\n",
    "    \"\"\"\n",
    "    Tries in this order:\n",
    "    1) <answer>...</answer>\n",
    "    2) line starting with #### (gsm8k)\n",
    "    3) last numeric/fraction token in response\n",
    "    4) fallback: last non-empty line\n",
    "    \"\"\"\n",
    "    if response is None:\n",
    "        return \"\"\n",
    "\n",
    "    text = str(response)\n",
    "\n",
    "    # cut off runaway turns if present\n",
    "    if \"<end_of_turn>\" in text:\n",
    "        text = text.split(\"<end_of_turn>\")[0]\n",
    "\n",
    "    # 1) <answer> tag\n",
    "    m = re.search(r\"<answer>\\s*(.*?)\\s*</answer>\", text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "\n",
    "    # 2) GSM8K #### final\n",
    "    m = re.search(r\"####\\s*(.+)\", text)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "\n",
    "    # 3) last fraction or number (keeps % too)\n",
    "    tokens = re.findall(r\"-?\\d+(?:\\.\\d+)?(?:/\\d+(?:\\.\\d+)?)?%?\", text)\n",
    "    if tokens:\n",
    "        return tokens[-1].strip()\n",
    "\n",
    "    # 4) fallback: last non-empty line\n",
    "    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "    return lines[-1] if lines else text.strip()\n",
    "\n",
    "def gold_to_accept_set(gold: str):\n",
    "    \"\"\"\n",
    "    Handles cases like '3 or 8' by allowing multiple correct answers.\n",
    "    \"\"\"\n",
    "    if gold is None:\n",
    "        return set()\n",
    "\n",
    "    g = normalize_text(gold)\n",
    "\n",
    "    # allow 'x or y' answers\n",
    "    if \" or \" in g:\n",
    "        parts = [p.strip() for p in g.split(\" or \") if p.strip()]\n",
    "        return set(parts)\n",
    "\n",
    "    return {g}\n",
    "\n",
    "def is_correct(model_final: str, gold: str) -> bool:\n",
    "    mf = normalize_text(model_final)\n",
    "    accept = gold_to_accept_set(gold)\n",
    "    if not accept:\n",
    "        return False  # if no gold, can't score\n",
    "    return mf in accept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Run baseline inference and log per-question results\n",
    "\n",
    "Loops over `(question, gold)` pairs and:\n",
    "- Builds the prompt with `generate_inference_prompt`.\n",
    "- Calls `generation_sampler` with near-deterministic decoding (`temperature=0.01`, `top_k=1`).\n",
    "- Extracts the final answer and checks correctness.\n",
    "- Stores a rich record per example:\n",
    "  - prompt, raw response, parsed answer, gold answer, correctness flag\n",
    "\n",
    "Outputs `df_res.sample(4)` for a quick spot check.\n",
    "\n",
    "**Tip**\n",
    "If you want more diverse reasoning, raise temperature (but that makes scoring noisier unless you use voting/self-consistency).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T02:50:11.430057Z",
     "iopub.status.busy": "2025-12-21T02:50:11.429765Z",
     "iopub.status.idle": "2025-12-21T02:51:51.064898Z",
     "shell.execute_reply": "2025-12-21T02:51:51.064085Z",
     "shell.execute_reply.started": "2025-12-21T02:50:11.430031Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.1 s, sys: 12.4 s, total: 1min\n",
      "Wall time: 1min 39s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>question</th>\n",
       "      <th>gold_answer</th>\n",
       "      <th>prompt</th>\n",
       "      <th>model_final_answer</th>\n",
       "      <th>model_raw_response</th>\n",
       "      <th>is_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>179</td>\n",
       "      <td>The population of a town increases by 5% annua...</td>\n",
       "      <td>8000</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n",
       "      <td>0.860777</td>\n",
       "      <td>We are given that the current population of a ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>171</td>\n",
       "      <td>Evaluate: 12 - [6 - {4 - (8 - 6) + 3}].</td>\n",
       "      <td>11</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n",
       "      <td>0</td>\n",
       "      <td>\\nLet's break down the expression step-by-step...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>130</td>\n",
       "      <td>The sum of interior angles of a pentagon is?</td>\n",
       "      <td>540°</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n",
       "      <td>180</td>\n",
       "      <td>The sum of interior angles of a polygon is alw...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>178</td>\n",
       "      <td>Price of sugar rises by 20%. By how much perce...</td>\n",
       "      <td>16.67%</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n",
       "      <td>1.20</td>\n",
       "      <td>To find the percent reduction in consumption n...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     idx                                           question gold_answer  \\\n",
       "178  179  The population of a town increases by 5% annua...        8000   \n",
       "170  171            Evaluate: 12 - [6 - {4 - (8 - 6) + 3}].          11   \n",
       "129  130       The sum of interior angles of a pentagon is?        540°   \n",
       "177  178  Price of sugar rises by 20%. By how much perce...      16.67%   \n",
       "\n",
       "                                                prompt model_final_answer  \\\n",
       "178  <start_of_turn>user\\nSolve the math problem. Y...           0.860777   \n",
       "170  <start_of_turn>user\\nSolve the math problem. Y...                  0   \n",
       "129  <start_of_turn>user\\nSolve the math problem. Y...                180   \n",
       "177  <start_of_turn>user\\nSolve the math problem. Y...               1.20   \n",
       "\n",
       "                                    model_raw_response  is_correct  \n",
       "178  We are given that the current population of a ...       False  \n",
       "170  \\nLet's break down the expression step-by-step...       False  \n",
       "129  The sum of interior angles of a polygon is alw...       False  \n",
       "177  To find the percent reduction in consumption n...       False  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# -----------------------------\n",
    "# 3) Run evaluation\n",
    "# -----------------------------\n",
    "results = []\n",
    "\n",
    "for i, (q, gold) in enumerate(zip(questions, golds), 1):\n",
    "    prompt = generate_inference_prompt(q)\n",
    "\n",
    "    out = generation_sampler(\n",
    "        input_strings=[prompt],\n",
    "        max_generation_steps=256,\n",
    "        temperature=0.01,\n",
    "        top_k=1,\n",
    "    )\n",
    "\n",
    "    response_raw = out.text[0]\n",
    "    model_final = extract_final_from_response(response_raw)\n",
    "\n",
    "    correct = None\n",
    "    if gold is not None:\n",
    "        correct = is_correct(model_final, gold)\n",
    "\n",
    "    results.append({\n",
    "        \"idx\": i,\n",
    "        \"question\": q,\n",
    "        \"gold_answer\": gold,\n",
    "        \"prompt\": prompt,\n",
    "        \"model_final_answer\": model_final,\n",
    "        \"model_raw_response\": response_raw,\n",
    "        \"is_correct\": correct\n",
    "    })\n",
    "\n",
    "df_res = pd.DataFrame(results)\n",
    "df_res.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T02:26:34.921546Z",
     "iopub.status.busy": "2025-12-21T02:26:34.921361Z",
     "iopub.status.idle": "2025-12-21T02:26:34.925245Z",
     "shell.execute_reply": "2025-12-21T02:26:34.924319Z",
     "shell.execute_reply.started": "2025-12-21T02:26:34.921528Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The present age of A is twice that of B. After 10 years, A will be 1.5 times B. What is A's present age?\n"
     ]
    }
   ],
   "source": [
    "print(df_res.iloc[36].question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Summarize baseline accuracy and surface failures\n",
    "\n",
    "Creates:\n",
    "- a one-row summary table (total, correct, wrong, accuracy)\n",
    "- `wrong_df`: a failure report including `prompt` and full `model_raw_response`\n",
    "\n",
    "**Why this is useful**\n",
    "When doing SFT, the fastest quality loop is:\n",
    "1) inspect failure modes,  \n",
    "2) adjust formatting/training data,  \n",
    "3) re-train,  \n",
    "4) re-evaluate with the same harness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T02:53:05.045057Z",
     "iopub.status.busy": "2025-12-21T02:53:05.044819Z",
     "iopub.status.idle": "2025-12-21T02:53:05.060492Z",
     "shell.execute_reply": "2025-12-21T02:53:05.059703Z",
     "shell.execute_reply.started": "2025-12-21T02:53:05.045038Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_scored</th>\n",
       "      <th>correct</th>\n",
       "      <th>wrong</th>\n",
       "      <th>accuracy_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>74</td>\n",
       "      <td>126</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_scored  correct  wrong  accuracy_%\n",
       "0           200       74    126        37.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>question</th>\n",
       "      <th>gold_answer</th>\n",
       "      <th>model_final_answer</th>\n",
       "      <th>prompt</th>\n",
       "      <th>model_raw_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A number is increased by 20% to become 360. Wh...</td>\n",
       "      <td>300</td>\n",
       "      <td>1800</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n",
       "      <td>We are given that a number is increased by 20%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>The average of 5 consecutive numbers is 64. Wh...</td>\n",
       "      <td>62</td>\n",
       "      <td>64</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n",
       "      <td>The problem states that the average of 5 conse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>The average age of 4 people is 30 years. When ...</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n",
       "      <td>Let the ages of the first four people be $a_1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>The ratio of boys to girls is 5:3. If 10 girls...</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n",
       "      <td>Let's analyze the problem step-by-step.\\nIniti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>The ratio of milk to water is 7:3. If 6 liters...</td>\n",
       "      <td>42 liters</td>\n",
       "      <td>6</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n",
       "      <td>Let $m$ be the initial amount of milk and $w$ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>193</td>\n",
       "      <td>Multiply: 0.2 x 0.3.</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.6</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n",
       "      <td>\\nTo multiply 0.2 by 0.3, we can multiply the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>194</td>\n",
       "      <td>Divide: 1.5 / 0.5.</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n",
       "      <td>\\nTo divide 1.5 by 0.5, we can rewrite the div...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>195</td>\n",
       "      <td>Reciprocal of 2 1/3 is?</td>\n",
       "      <td>3/7</td>\n",
       "      <td>9</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n",
       "      <td>\\nTo find the reciprocal of 2 1/3, we need to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>199</td>\n",
       "      <td>Greatest number that divides 43, 91 and 183 le...</td>\n",
       "      <td>4</td>\n",
       "      <td>43</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n",
       "      <td>To find the greatest number that divides 43, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>200</td>\n",
       "      <td>Three bells ring at intervals of 10, 15, 20 mi...</td>\n",
       "      <td>11 AM</td>\n",
       "      <td>26:10</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n",
       "      <td>The bells ring at intervals of 10, 15, and 20 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     idx                                           question gold_answer  \\\n",
       "0      1  A number is increased by 20% to become 360. Wh...         300   \n",
       "1      5  The average of 5 consecutive numbers is 64. Wh...          62   \n",
       "2      6  The average age of 4 people is 30 years. When ...          40   \n",
       "3      7  The ratio of boys to girls is 5:3. If 10 girls...          50   \n",
       "4      8  The ratio of milk to water is 7:3. If 6 liters...   42 liters   \n",
       "..   ...                                                ...         ...   \n",
       "121  193                               Multiply: 0.2 x 0.3.        0.06   \n",
       "122  194                                 Divide: 1.5 / 0.5.           3   \n",
       "123  195                            Reciprocal of 2 1/3 is?         3/7   \n",
       "124  199  Greatest number that divides 43, 91 and 183 le...           4   \n",
       "125  200  Three bells ring at intervals of 10, 15, 20 mi...       11 AM   \n",
       "\n",
       "    model_final_answer                                             prompt  \\\n",
       "0                 1800  <start_of_turn>user\\nSolve the math problem. Y...   \n",
       "1                   64  <start_of_turn>user\\nSolve the math problem. Y...   \n",
       "2                    2  <start_of_turn>user\\nSolve the math problem. Y...   \n",
       "3                   25  <start_of_turn>user\\nSolve the math problem. Y...   \n",
       "4                    6  <start_of_turn>user\\nSolve the math problem. Y...   \n",
       "..                 ...                                                ...   \n",
       "121                0.6  <start_of_turn>user\\nSolve the math problem. Y...   \n",
       "122                3.0  <start_of_turn>user\\nSolve the math problem. Y...   \n",
       "123                  9  <start_of_turn>user\\nSolve the math problem. Y...   \n",
       "124                 43  <start_of_turn>user\\nSolve the math problem. Y...   \n",
       "125              26:10  <start_of_turn>user\\nSolve the math problem. Y...   \n",
       "\n",
       "                                    model_raw_response  \n",
       "0    We are given that a number is increased by 20%...  \n",
       "1    The problem states that the average of 5 conse...  \n",
       "2    Let the ages of the first four people be $a_1,...  \n",
       "3    Let's analyze the problem step-by-step.\\nIniti...  \n",
       "4    Let $m$ be the initial amount of milk and $w$ ...  \n",
       "..                                                 ...  \n",
       "121  \\nTo multiply 0.2 by 0.3, we can multiply the ...  \n",
       "122  \\nTo divide 1.5 by 0.5, we can rewrite the div...  \n",
       "123  \\nTo find the reciprocal of 2 1/3, we need to ...  \n",
       "124  To find the greatest number that divides 43, 9...  \n",
       "125  The bells ring at intervals of 10, 15, and 20 ...  \n",
       "\n",
       "[126 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.97 ms, sys: 917 μs, total: 10.9 ms\n",
      "Wall time: 10.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# -----------------------------\n",
    "# 4) Summary tables\n",
    "# -----------------------------\n",
    "if df_res[\"is_correct\"].notna().any():\n",
    "    total = df_res[\"is_correct\"].notna().sum()\n",
    "    correct_n = int((df_res[\"is_correct\"] == True).sum())\n",
    "    wrong_n = int((df_res[\"is_correct\"] == False).sum())\n",
    "    acc = correct_n / total if total else 0.0\n",
    "\n",
    "    summary = pd.DataFrame([{\n",
    "        \"total_scored\": total,\n",
    "        \"correct\": correct_n,\n",
    "        \"wrong\": wrong_n,\n",
    "        \"accuracy_%\": round(acc * 100, 2),\n",
    "    }])\n",
    "    display(summary)\n",
    "\n",
    "    wrong_df = df_res[df_res[\"is_correct\"] == False][\n",
    "        [\"idx\", \"question\", \"gold_answer\", \"model_final_answer\", \"prompt\", \"model_raw_response\"]\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "    display(wrong_df)\n",
    "else:\n",
    "    print(\"No gold answers were loaded, so scoring is skipped.\")\n",
    "    display(df_res[[\"idx\", \"question\", \"prompt\", \"model_final_answer\", \"model_raw_response\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11: Quick look at wrong predictions\n",
    "\n",
    "Displays `wrong_df.head()` so you can immediately inspect the first few mistakes with:\n",
    "- the question\n",
    "- the expected answer\n",
    "- the model’s parsed final answer\n",
    "- the full prompt and raw completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T02:53:13.507229Z",
     "iopub.status.busy": "2025-12-21T02:53:13.506896Z",
     "iopub.status.idle": "2025-12-21T02:53:13.515147Z",
     "shell.execute_reply": "2025-12-21T02:53:13.514265Z",
     "shell.execute_reply.started": "2025-12-21T02:53:13.507203Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>question</th>\n",
       "      <th>gold_answer</th>\n",
       "      <th>model_final_answer</th>\n",
       "      <th>prompt</th>\n",
       "      <th>model_raw_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A number is increased by 20% to become 360. Wh...</td>\n",
       "      <td>300</td>\n",
       "      <td>1800</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n",
       "      <td>We are given that a number is increased by 20%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>The average of 5 consecutive numbers is 64. Wh...</td>\n",
       "      <td>62</td>\n",
       "      <td>64</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n",
       "      <td>The problem states that the average of 5 conse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>The average age of 4 people is 30 years. When ...</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n",
       "      <td>Let the ages of the first four people be $a_1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>The ratio of boys to girls is 5:3. If 10 girls...</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n",
       "      <td>Let's analyze the problem step-by-step.\\nIniti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>The ratio of milk to water is 7:3. If 6 liters...</td>\n",
       "      <td>42 liters</td>\n",
       "      <td>6</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n",
       "      <td>Let $m$ be the initial amount of milk and $w$ ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx                                           question gold_answer  \\\n",
       "0    1  A number is increased by 20% to become 360. Wh...         300   \n",
       "1    5  The average of 5 consecutive numbers is 64. Wh...          62   \n",
       "2    6  The average age of 4 people is 30 years. When ...          40   \n",
       "3    7  The ratio of boys to girls is 5:3. If 10 girls...          50   \n",
       "4    8  The ratio of milk to water is 7:3. If 6 liters...   42 liters   \n",
       "\n",
       "  model_final_answer                                             prompt  \\\n",
       "0               1800  <start_of_turn>user\\nSolve the math problem. Y...   \n",
       "1                 64  <start_of_turn>user\\nSolve the math problem. Y...   \n",
       "2                  2  <start_of_turn>user\\nSolve the math problem. Y...   \n",
       "3                 25  <start_of_turn>user\\nSolve the math problem. Y...   \n",
       "4                  6  <start_of_turn>user\\nSolve the math problem. Y...   \n",
       "\n",
       "                                  model_raw_response  \n",
       "0  We are given that a number is increased by 20%...  \n",
       "1  The problem states that the average of 5 conse...  \n",
       "2  Let the ages of the first four people be $a_1,...  \n",
       "3  Let's analyze the problem step-by-step.\\nIniti...  \n",
       "4  Let $m$ be the initial amount of milk and $w$ ...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre- Fine tuning model process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 12: GSM8K answer extraction helper (`#### ...`)\n",
    "\n",
    "Defines a helper to extract the final numeric answer from GSM8K examples, which commonly use the pattern:\n",
    "\n",
    "`... #### 42`\n",
    "\n",
    "**Why it matters**\n",
    "You need a reliable way to obtain the “gold” final answer so you can build supervised `<answer>...</answer>` targets for SFT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T02:53:58.676016Z",
     "iopub.status.busy": "2025-12-21T02:53:58.675685Z",
     "iopub.status.idle": "2025-12-21T02:53:59.578129Z",
     "shell.execute_reply": "2025-12-21T02:53:59.577308Z",
     "shell.execute_reply.started": "2025-12-21T02:53:58.676000Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GSM8K dataset...\n",
      "✓ Loaded 7473 training examples\n",
      "✓ Loaded 1319 test examples\n",
      "\n",
      "Example question:\n",
      "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "\n",
      "Example answer:\n",
      "Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
      "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
      "#### 72\n",
      "\n",
      "Extracted reasoning:\n",
      "Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
      "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
      "\n",
      "Extracted numerical answer:\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "# Helper function to extract answer from GSM8K format\n",
    "def extract_hash_answer(text):\n",
    "    \"\"\"Extract numerical answer after #### delimiter.\"\"\"\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "\n",
    "# Helper function to extract reasoning from GSM8K format\n",
    "def extract_reasoning(text):\n",
    "    \"\"\"Extract reasoning (everything before #### delimiter).\"\"\"\n",
    "    if \"####\" not in text:\n",
    "        return text.strip()\n",
    "    return text.split(\"####\")[0].strip()\n",
    "\n",
    "# Load GSM8K dataset\n",
    "print(\"Loading GSM8K dataset...\")\n",
    "train_dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\")\n",
    "test_dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
    "print(f\"✓ Loaded {len(train_dataset)} training examples\")\n",
    "print(f\"✓ Loaded {len(test_dataset)} test examples\")\n",
    "\n",
    "\n",
    "print(\"\\nExample question:\")\n",
    "print(train_dataset[0][\"question\"])\n",
    "print(\"\\nExample answer:\")\n",
    "print(train_dataset[0][\"answer\"])\n",
    "print(\"\\nExtracted reasoning:\")\n",
    "print(extract_reasoning(train_dataset[0][\"answer\"]))\n",
    "print(\"\\nExtracted numerical answer:\")\n",
    "print(extract_hash_answer(train_dataset[0][\"answer\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 13: Load GSM8K and format into strict SFT training examples\n",
    "\n",
    "- Loads GSM8K train/test splits via `datasets.load_dataset`.\n",
    "- Defines `clean_gsm8k_content()` to remove/normalize GSM8K-specific artifacts like `<<10+5=15>>`.\n",
    "- Defines `format_gsm8k_example(ex)` to build a single training string in Gemma chat format:\n",
    "  - user turn: system prompt + question\n",
    "  - model turn: `<reasoning> cleaned reasoning </reasoning>` + `<answer> extracted answer </answer>`\n",
    "\n",
    "Produces:\n",
    "- `formatted_train`: list of dicts with `{\"text\": ...}`\n",
    "- `formatted_test`: same for evaluation\n",
    "\n",
    "**Why this works**\n",
    "SFT teaches the model to imitate the “ideal completion” for the exact prompt you will use at inference time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T02:54:05.791594Z",
     "iopub.status.busy": "2025-12-21T02:54:05.791303Z",
     "iopub.status.idle": "2025-12-21T02:54:05.994332Z",
     "shell.execute_reply": "2025-12-21T02:54:05.993369Z",
     "shell.execute_reply.started": "2025-12-21T02:54:05.791549Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refining dataset with CLEANING and STRICT System Prompt...\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "reasoning_start = \"<reasoning>\"\n",
    "reasoning_end = \"</reasoning>\"\n",
    "solution_start = \"<answer>\"\n",
    "solution_end = \"</answer>\"\n",
    "\n",
    "\n",
    "# 1. Define the Cleaning Helper\n",
    "def clean_gsm8k_content(text):\n",
    "    \"\"\"\n",
    "    Removes GSM8K specific calculation annotations.\n",
    "    Converts '<<10+5=15>>' to '(10+5=15)' or just removes them if preferred.\n",
    "    For SFT, replacing with parentheses is usually safer than deleting.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    # Replace << and >> with parentheses to make it standard math text\n",
    "    cleaned = text.replace(\"<<\", \"(\").replace(\">>\", \")\")\n",
    "    return cleaned\n",
    "\n",
    "# 2. Define the Formatter\n",
    "def format_gsm8k_example(example):\n",
    "    \"\"\"\n",
    "    Formats training data with strict system instructions and data cleaning.\n",
    "    \"\"\"\n",
    "    question = example[\"question\"]\n",
    "    raw_answer = example[\"answer\"]\n",
    "    \n",
    "    # Extract parts\n",
    "    reasoning = extract_reasoning(raw_answer)\n",
    "    answer = extract_hash_answer(raw_answer)\n",
    "    \n",
    "    # --- APPLY CLEANING HERE ---\n",
    "    # We clean the reasoning part because that's where the <<...>> artifacts live.\n",
    "    #reasoning = clean_gsm8k_content(reasoning)\n",
    "    \n",
    "    # --- PROMPT CONSTRUCTION ---\n",
    "    \n",
    "    # 1. User Turn (Includes the strict instructions)\n",
    "    text = f\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n\\n{question}<end_of_turn>\\n\"\n",
    "    \n",
    "    # 2. Model Turn (The expected strict output)\n",
    "    text += f\"<start_of_turn>model\\n\"\n",
    "    text += f\"<reasoning>\\n{reasoning}\\n</reasoning>\\n\"\n",
    "    text += f\"<answer>\\n{answer}\\n</answer>\"\n",
    "    text += f\"<end_of_turn>\"\n",
    "\n",
    "    return {\"text\": text}\n",
    "\n",
    "print(\"Refining dataset with CLEANING and STRICT System Prompt...\")\n",
    "formatted_train = [format_gsm8k_example(ex) for ex in train_dataset]\n",
    "formatted_test = [format_gsm8k_example(ex) for ex in test_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 14: Print a sample formatted example\n",
    "\n",
    "Prints one formatted training example so you can validate:\n",
    "- the chat markers (`<start_of_turn>...`)\n",
    "- the system prompt presence\n",
    "- reasoning and answer tags\n",
    "- absence of GSM8K artifacts\n",
    "\n",
    "This is a critical sanity check before launching a TPU training run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T02:54:16.667847Z",
     "iopub.status.busy": "2025-12-21T02:54:16.667583Z",
     "iopub.status.idle": "2025-12-21T02:54:16.671860Z",
     "shell.execute_reply": "2025-12-21T02:54:16.670821Z",
     "shell.execute_reply.started": "2025-12-21T02:54:16.667826Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "<start_of_turn>user\n",
      "Solve the math problem. You must STRICTLY follow this format:\n",
      "1. Enclose your step-by-step logic inside <reasoning>...</reasoning> tags.\n",
      "2. Enclose the final numerical result inside <answer>...</answer> tags.\n",
      "\n",
      "A craft store makes a third of its sales in the fabric section, a quarter of its sales in the jewelry section, and the rest in the stationery section. They made 36 sales today. How many sales were in the stationery section?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "<reasoning>\n",
      "The craft store made 36 / 3 = (36/3=12)12 sales in the fabric section.\n",
      "It made 36 / 4 = (36/4=9)9 sales in the jewelry section.\n",
      "Thus, there were 36 - 12 - 9 = (36-12-9=15)15 sales in the stationery section.\n",
      "</reasoning>\n",
      "<answer>\n",
      "15\n",
      "</answer><end_of_turn>\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-\" * 60)\n",
    "print(formatted_train[100][\"text\"])\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 15: Tokenization + Grain input pipelines (train/eval)\n",
    "\n",
    "- Defines `tokenize_function(example)` which:\n",
    "  - tokenizes the full supervised text\n",
    "  - separately tokenizes the **prompt prefix** up to `<start_of_turn>model\\n`\n",
    "  - builds masks so loss is applied primarily to the model completion portion (common SFT practice)\n",
    "  - pads/truncates to `MAX_SEQ_LENGTH`\n",
    "  - wraps everything into Tunix `TrainingInput`\n",
    "- Builds Grain datasets:\n",
    "  - shuffle + repeat for training\n",
    "  - batch for train and eval\n",
    "\n",
    "**Why Grain**\n",
    "Grain is optimized for JAX input pipelines and plays well with TPU training.\n",
    "\n",
    "**Note**\n",
    "This cell contains `...` placeholders in the notebook source. Ensure the mask/padding logic is complete and executable before running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T02:54:21.349078Z",
     "iopub.status.busy": "2025-12-21T02:54:21.348800Z",
     "iopub.status.idle": "2025-12-21T02:54:21.355955Z",
     "shell.execute_reply": "2025-12-21T02:54:21.355068Z",
     "shell.execute_reply.started": "2025-12-21T02:54:21.349060Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Train batches: 37,365\n",
      "✓ Eval batches: 659\n"
     ]
    }
   ],
   "source": [
    "import grain.python as grain\n",
    "import numpy as np\n",
    "from tunix.sft.peft_trainer import TrainingInput\n",
    "\n",
    "def tokenize_function(example):\n",
    "    full_text = example[\"text\"]\n",
    "    full_tokens = tokenizer.encode(full_text)\n",
    "    \n",
    "    \n",
    "    prompt_text = full_text.split(\"<start_of_turn>model\")[0] + \"<start_of_turn>model\\n\"\n",
    "    prompt_tokens = tokenizer.encode(prompt_text)\n",
    "    prompt_len = len(prompt_tokens)\n",
    "\n",
    "    # Padding/Truncation Logic\n",
    "    if len(full_tokens) > MAX_SEQ_LENGTH:\n",
    "        full_tokens = full_tokens[:MAX_SEQ_LENGTH]\n",
    "    else:\n",
    "        pad_token = tokenizer.pad_id() if hasattr(tokenizer, 'pad_id') else tokenizer.eos_id()\n",
    "        full_tokens = full_tokens + [pad_token] * (MAX_SEQ_LENGTH - len(full_tokens))\n",
    "\n",
    "    input_tokens = np.array(full_tokens, dtype=np.int32)\n",
    "    \n",
    "    # Create Mask\n",
    "    loss_mask = np.zeros_like(input_tokens, dtype=np.float32)\n",
    "    \n",
    "    # Enable loss only for the response part (ignoring padding)\n",
    "    seq_len = min(len(tokenizer.encode(full_text)), MAX_SEQ_LENGTH)\n",
    "    if seq_len > prompt_len:\n",
    "        loss_mask[prompt_len:seq_len] = 1.0\n",
    "\n",
    "    return TrainingInput(input_tokens=input_tokens, input_mask=loss_mask)\n",
    "\n",
    "\n",
    "# Create Grain datasets\n",
    "train_grain = (\n",
    "    grain.MapDataset.source(formatted_train)\n",
    "    .map(tokenize_function)\n",
    "    .shuffle(seed=42)\n",
    "    .repeat(NUM_EPOCHS)\n",
    "    .batch(batch_size=TRAIN_MICRO_BATCH_SIZE, drop_remainder=True)\n",
    ")\n",
    "\n",
    "eval_grain = (\n",
    "    grain.MapDataset.source(formatted_test)\n",
    "    .map(tokenize_function)\n",
    "    .batch(batch_size=TRAIN_MICRO_BATCH_SIZE, drop_remainder=True)\n",
    ")\n",
    "\n",
    "print(f\"✓ Train batches: {len(train_grain):,}\")\n",
    "print(f\"✓ Eval batches: {len(eval_grain):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 16: Learning-rate schedule and optimizer (Optax)\n",
    "\n",
    "- Builds a warmup + cosine decay LR schedule.\n",
    "- Creates an optimizer chain:\n",
    "  1) global norm clipping (stability)\n",
    "  2) Adam moments\n",
    "  3) weight decay (regularization)\n",
    "  4) scheduled LR scaling\n",
    "  5) negative scale to perform gradient descent\n",
    "\n",
    "Prints the final optimizer settings for auditability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T02:54:25.507199Z",
     "iopub.status.busy": "2025-12-21T02:54:25.506938Z",
     "iopub.status.idle": "2025-12-21T02:54:25.512072Z",
     "shell.execute_reply": "2025-12-21T02:54:25.511228Z",
     "shell.execute_reply.started": "2025-12-21T02:54:25.507182Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Optimizer configured:\n",
      "  Learning rate: 2e-05\n",
      "  Warmup steps: 50\n",
      "  Total steps: 3000\n",
      "  Weight decay: 0.01\n",
      "  Max grad norm: 1.0\n"
     ]
    }
   ],
   "source": [
    "import optax\n",
    "\n",
    "schedule = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=0.0,\n",
    "    peak_value=LEARNING_RATE,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    decay_steps=MAX_STEPS - WARMUP_STEPS,\n",
    "    end_value=LEARNING_RATE * 0.1,\n",
    ")\n",
    "\n",
    "# Create optimizer chain\n",
    "optimizer = optax.chain(\n",
    "    optax.clip_by_global_norm(MAX_GRAD_NORM),\n",
    "    optax.scale_by_adam(\n",
    "        b1=ADAM_BETA1,\n",
    "        b2=ADAM_BETA2,\n",
    "        eps=ADAM_EPSILON,\n",
    "    ),\n",
    "    optax.add_decayed_weights(WEIGHT_DECAY),\n",
    "    optax.scale_by_schedule(schedule),\n",
    "    optax.scale(-1.0),  # Gradient descent\n",
    ")\n",
    "\n",
    "print(\"✓ Optimizer configured:\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Warmup steps: {WARMUP_STEPS}\")\n",
    "print(f\"  Total steps: {MAX_STEPS}\")\n",
    "print(f\"  Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"  Max grad norm: {MAX_GRAD_NORM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training part starts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 17: Trainer configuration (checkpoints, logging, model input fn)\n",
    "\n",
    "- Configures Orbax checkpoint manager:\n",
    "  - save cadence\n",
    "  - retention policy (`max_to_keep`)\n",
    "- Builds a `TrainingConfig`:\n",
    "  - total steps, eval cadence, gradient accumulation, checkpoint/log directories\n",
    "  - TensorBoard metric logging\n",
    "- Defines `gen_model_input_fn(training_input)`:\n",
    "  - builds `positions` and causal `attention_mask` from non-padding tokens\n",
    "  - returns the dict expected by the Gemma model forward pass\n",
    "- Instantiates `PeftTrainer` and attaches the input adapter.\n",
    "\n",
    "**Important nuance**\n",
    "Despite the class name `PeftTrainer`, this notebook’s print statements suggest full fine-tuning. Confirm whether PEFT adapters are actually enabled; otherwise this is “full-parameter” training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T02:54:30.457055Z",
     "iopub.status.busy": "2025-12-21T02:54:30.456780Z",
     "iopub.status.idle": "2025-12-21T02:54:32.685419Z",
     "shell.execute_reply": "2025-12-21T02:54:32.684170Z",
     "shell.execute_reply.started": "2025-12-21T02:54:30.457035Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training configuration created\n",
      "  Max steps: 3000\n",
      "  Micro batch size: 2\n",
      "  Gradient accumulation: 4\n",
      "  Effective batch size: 8\n",
      "  Eval interval: 50\n",
      "  Save interval: 100\n",
      "✓ Trainer ready for training\n",
      "  Model: Gemma 3 1B (Full Fine-Tuning)\n",
      "  Max steps: 3000\n"
     ]
    }
   ],
   "source": [
    "from tunix import PeftTrainer, TrainingConfig, MetricsLoggerOptions\n",
    "import orbax.checkpoint as ocp\n",
    "\n",
    "checkpointing_options = ocp.CheckpointManagerOptions(\n",
    "    save_interval_steps=SAVE_INTERVAL_STEPS,\n",
    "    max_to_keep=3,  # Keep last 3 checkpoints\n",
    ")\n",
    "\n",
    "training_config = TrainingConfig(\n",
    "    max_steps=MAX_STEPS,\n",
    "    eval_every_n_steps=EVAL_INTERVAL_STEPS,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    checkpoint_root_directory=CHECKPOINT_DIR,\n",
    "    checkpointing_options=checkpointing_options,\n",
    "    metrics_logging_options=MetricsLoggerOptions(\n",
    "        log_dir=TENSORBOARD_DIR,\n",
    "        flush_every_n_steps=LOG_INTERVAL_STEPS\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"✓ Training configuration created\")\n",
    "print(f\"  Max steps: {MAX_STEPS}\")\n",
    "print(f\"  Micro batch size: {TRAIN_MICRO_BATCH_SIZE}\")\n",
    "print(f\"  Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  Effective batch size: {TRAIN_MICRO_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  Eval interval: {EVAL_INTERVAL_STEPS}\")\n",
    "print(f\"  Save interval: {SAVE_INTERVAL_STEPS}\")\n",
    "\n",
    "# Model input function\n",
    "from tunix.sft import utils\n",
    "\n",
    "def gen_model_input_fn(training_input):\n",
    "    \"\"\"Convert TrainingInput to model-compatible format.\"\"\"\n",
    "    pad_mask = training_input.input_tokens != 0\n",
    "    positions = utils.build_positions_from_mask(pad_mask)\n",
    "    attention_mask = utils.make_causal_attn_mask(pad_mask)\n",
    "    \n",
    "    return {\n",
    "        'input_tokens': training_input.input_tokens,\n",
    "        'input_mask': training_input.input_mask,\n",
    "        'positions': positions,\n",
    "        'attention_mask': attention_mask,\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = PeftTrainer(\n",
    "    model=gemma3_model,\n",
    "    optimizer=optimizer,\n",
    "    training_config=training_config,\n",
    ")\n",
    "trainer = trainer.with_gen_model_input_fn(gen_model_input_fn)\n",
    "\n",
    "print(\"✓ Trainer ready for training\")\n",
    "print(f\"  Model: Gemma 3 1B (Full Fine-Tuning)\")\n",
    "print(f\"  Max steps: {MAX_STEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 18: Launch training and verify TPU usage via timing\n",
    "\n",
    "- Prints run metadata (steps, dataset sizes, batch and accumulation).\n",
    "- Re-checks parameter device placement (TPU vs CPU).\n",
    "- Calls `trainer.train(train_ds=..., eval_ds=...)`.\n",
    "- Reports total time, time per step, and checkpoint location.\n",
    "- Includes a heuristic “TPU vs CPU” check based on average step time after compilation.\n",
    "\n",
    "**Caveat**\n",
    "The timing heuristic is rough; the first few steps include XLA compilation. For a more reliable check, confirm device placement and look at TPU utilization in the runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T02:54:41.857072Z",
     "iopub.status.busy": "2025-12-21T02:54:41.856788Z",
     "iopub.status.idle": "2025-12-21T02:56:51.229215Z",
     "shell.execute_reply": "2025-12-21T02:56:51.227930Z",
     "shell.execute_reply.started": "2025-12-21T02:54:41.857051Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Starting Full Fine-Tuning on TPU v5e-8\n",
      "============================================================\n",
      "Max steps: 3000\n",
      "Training examples: 7473\n",
      "Eval examples: 1319\n",
      "Batch size: 2\n",
      "Gradient accumulation: 4\n",
      "============================================================\n",
      "✓ Model parameters are on: TPU v5 lite\n",
      "✓✓✓ CONFIRMED: Model is ready for TPU training!\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "IMPORTANT: First training step will take 2-5 minutes\n",
      "============================================================\n",
      "JAX is compiling all functions (happens on CPU).\n",
      "After first step completes, TPU will be used and steps will be MUCH faster.\n",
      "You should see 'Compiling...' messages initially.\n",
      "============================================================\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1829d38427c4b129ff7476e5b66aee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 100%|##########| 3000/3000 [00:00<?, ?step/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training Completed!\n",
      "============================================================\n",
      "Total training time: 129.0 seconds (2.1 minutes)\n",
      "Average time per step: 0.0 seconds\n",
      "Checkpoints saved to: /kaggle/working/outputs_sft_full/checkpoints\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "POST-TRAINING: Verify TPU was used\n",
      "============================================================\n",
      "Expected TPU time: 5-15 seconds per step after compilation\n",
      "Your average: 0.0 seconds per step\n",
      "❌ WARNING: Training ran on CPU, not TPU!\n",
      "Results will be incorrect. Check that model is properly sharded.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "### training the models\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Starting Full Fine-Tuning on TPU v5e-8\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Max steps: {MAX_STEPS}\")\n",
    "print(f\"Training examples: {len(formatted_train)}\")\n",
    "print(f\"Eval examples: {len(formatted_test)}\")\n",
    "print(f\"Batch size: {TRAIN_MICRO_BATCH_SIZE}\")\n",
    "print(f\"Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "all_params = nnx.state(gemma3_model)\n",
    "param_leaves = jax.tree_util.tree_leaves(all_params)\n",
    "if len(param_leaves) > 0:\n",
    "    sample_param = param_leaves[0]\n",
    "    if hasattr(sample_param, 'devices'):\n",
    "        devices = sample_param.devices()\n",
    "        if len(devices) > 0:\n",
    "            device_kind = list(devices)[0].device_kind\n",
    "            print(f\"✓ Model parameters are on: {device_kind}\")\n",
    "            if 'tpu' not in device_kind.lower():\n",
    "                print(f\"⚠️  WARNING: Model params on {device_kind}, not TPU!\")\n",
    "                print(f\"⚠️  Training will run on CPU and produce wrong results!\")\n",
    "            else:\n",
    "                print(f\"✓✓✓ CONFIRMED: Model is ready for TPU training!\")\n",
    "        else:\n",
    "            print(\"⚠️  No devices found for model parameters\")\n",
    "    else:\n",
    "        print(\"⚠️  Cannot check device placement\")\n",
    "else:\n",
    "    print(\"⚠️  No model parameters found\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"IMPORTANT: First training step will take 2-5 minutes\")\n",
    "print(\"=\"*60)\n",
    "print(\"JAX is compiling all functions (happens on CPU).\")\n",
    "print(\"After first step completes, TPU will be used and steps will be MUCH faster.\")\n",
    "print(\"You should see 'Compiling...' messages initially.\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "trainer.train(\n",
    "    train_ds=train_grain,\n",
    "    eval_ds=eval_grain,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Completed!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total training time: {total_time:.1f} seconds ({total_time/60:.1f} minutes)\")\n",
    "print(f\"Average time per step: {total_time/MAX_STEPS:.1f} seconds\")\n",
    "print(f\"Checkpoints saved to: {CHECKPOINT_DIR}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"POST-TRAINING: Verify TPU was used\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Expected TPU time: 5-15 seconds per step after compilation\")\n",
    "print(f\"Your average: {total_time/MAX_STEPS:.1f} seconds per step\")\n",
    "if total_time/MAX_STEPS < 1.0:\n",
    "    print(\"❌ WARNING: Training ran on CPU, not TPU!\")\n",
    "    print(\"Results will be incorrect. Check that model is properly sharded.\")\n",
    "else:\n",
    "    print(\"✓ Training timing looks correct for TPU usage!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 19: Rebuild the generation sampler for the fine-tuned model\n",
    "\n",
    "After training, you typically re-instantiate:\n",
    "- cache config\n",
    "- `Sampler`\n",
    "\n",
    "This ensures generation uses the updated in-memory weights and a fresh cache, then you can re-run evaluation with the same parsing/scoring code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T02:47:01.199370Z",
     "iopub.status.busy": "2025-12-21T02:47:01.199038Z",
     "iopub.status.idle": "2025-12-21T02:47:01.237982Z",
     "shell.execute_reply": "2025-12-21T02:47:01.236912Z",
     "shell.execute_reply.started": "2025-12-21T02:47:01.199345Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tunix.generate import sampler as sampler_lib\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "cache_config = sampler_lib.CacheConfig(\n",
    "    cache_size=MAX_SEQ_LENGTH + 512,\n",
    "    num_layers=model_config.num_layers,\n",
    "    num_kv_heads=model_config.num_kv_heads,\n",
    "    head_dim=model_config.head_dim,\n",
    ")\n",
    "\n",
    "\n",
    "generation_sampler = sampler_lib.Sampler(\n",
    "    transformer=gemma3_model,\n",
    "    tokenizer=tokenizer,\n",
    "    cache_config=cache_config,\n",
    ")\n",
    "\n",
    "\n",
    "def generate_inference_prompt(question):\n",
    "    # Match the training exactly: Same System Prompt, No One-Shot needed anymore.\n",
    "    text = f\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n\\n{question}<end_of_turn>\\n\"\n",
    "    text += f\"<start_of_turn>model\\n<reasoning>\\n\" \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 20: Define test questions for quick smoke testing\n",
    "\n",
    "Creates a small list of arithmetic/word problems to validate:\n",
    "- the model follows the `<reasoning>` and `<answer>` format\n",
    "- the fine-tuned model improved on the types of questions you care about\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T02:47:05.832274Z",
     "iopub.status.busy": "2025-12-21T02:47:05.832027Z",
     "iopub.status.idle": "2025-12-21T02:47:21.983015Z",
     "shell.execute_reply": "2025-12-21T02:47:21.981904Z",
     "shell.execute_reply.started": "2025-12-21T02:47:05.832255Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Testing Trained Model (Strict Format)\n",
      "============================================================\n",
      "\n",
      "[Test 1] Question: What is the square root of 144?\n",
      "------------------------------------------------------------\n",
      "Response:\n",
      "</reasoning>\n",
      "The square root of 144 is 12 because 12*12=(12*12=144)144\n",
      "</reasoning>\n",
      "<answer>\n",
      "12\n",
      "</answer>\n",
      "============================================================\n",
      "\n",
      "[Test 2] Question: If a shirt costs $25 and is on sale for 20% off, what is the sale price?\n",
      "------------------------------------------------------------\n",
      "Response:\n",
      "reasoning>\n",
      "The discount for the shirt is $25 x 20/100 = $(25*20/100=5)5.\n",
      "So the shirt is sold for $25 - $5 = $(25-5=20)20.\n",
      "</reasoning>\n",
      "<answer>\n",
      "20\n",
      "</answer>\n",
      "============================================================\n",
      "\n",
      "[Test 3] Question: A train travels 60 miles in 45 minutes. What is its speed in miles per hour?\n",
      "------------------------------------------------------------\n",
      "Response:\n",
      "45 minutes is 45/60 = (45/60=0.75)0.75 hours.\n",
      "So the speed is 60/0.75 = (60/0.75=80)80 mph.\n",
      "</reasoning>\n",
      "<answer>\n",
      "80\n",
      "</answer>\n",
      "============================================================\n",
      "\n",
      "[Test 4] Question: What is 15% of 200?\n",
      "------------------------------------------------------------\n",
      "Response:\n",
      "reasoning>\n",
      "15% of 200 is (15/100)*200 = (15*200/100=30)30.\n",
      "</reasoning>\n",
      "<answer>\n",
      "30\n",
      "</answer>\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test questions\n",
    "test_questions = [\n",
    "    \"What is the square root of 144?\",\n",
    "    \"If a shirt costs $25 and is on sale for 20% off, what is the sale price?\",\n",
    "    \"A train travels 60 miles in 45 minutes. What is its speed in miles per hour?\",\n",
    "    \"What is 15% of 200?\",\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Testing Trained Model (Strict Format)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    # 1. Generate the formatted prompt\n",
    "    prompt = generate_inference_prompt(question)\n",
    "\n",
    "    print(f\"\\n[Test {i}] Question: {question}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # 2. Run Generation\n",
    "    sampler_output = generation_sampler(\n",
    "        input_strings=[prompt],\n",
    "        max_generation_steps=512,\n",
    "        temperature=0.01,  # Near-greedy for math\n",
    "        top_k=1,\n",
    "    )\n",
    "\n",
    "    # 3. Extract and Clean Response\n",
    "    response = sampler_output.text[0]\n",
    "    \n",
    "    # Manual Stop: Cut off text if the model generates <end_of_turn>\n",
    "    # This fixes the looping issue seen in Test 4\n",
    "    if \"<end_of_turn>\" in response:\n",
    "        response = response.split(\"<end_of_turn>\")[0]\n",
    "\n",
    "    print(f\"Response:\\n{response}\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T02:47:22.857057Z",
     "iopub.status.busy": "2025-12-21T02:47:22.856859Z",
     "iopub.status.idle": "2025-12-21T02:47:23.601939Z",
     "shell.execute_reply": "2025-12-21T02:47:23.600898Z",
     "shell.execute_reply.started": "2025-12-21T02:47:22.857041Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Testing Trained Model (Strict Format)\n",
      "============================================================\n",
      "\n",
      "[Test 1] Question: The present age of A is twice that of B. After 10 years, A will be 1.5 times B. What is A's present age?\n",
      "------------------------------------------------------------\n",
      "Response:\n",
      "\n",
      "============================================================\n",
      "\n",
      "[Test 2] Question: If a shirt costs $25 and is on sale for 20% off, what is the sale price?\n",
      "------------------------------------------------------------\n",
      "Response:\n",
      "reasoning>\n",
      "The discount for the shirt is $25 x 20/100 = $(25*20/100=5)5.\n",
      "So the shirt is sold for $25 - $5 = $(25-5=20)20.\n",
      "</reasoning>\n",
      "<answer>\n",
      "20\n",
      "</answer>\n",
      "============================================================\n",
      "\n",
      "[Test 3] Question: A train travels 60 miles in 45 minutes. What is its speed in miles per hour?\n",
      "------------------------------------------------------------\n",
      "Response:\n",
      "45 minutes is 45/60 = (45/60=0.75)0.75 hours.\n",
      "So the speed is 60/0.75 = (60/0.75=80)80 mph.\n",
      "</reasoning>\n",
      "<answer>\n",
      "80\n",
      "</answer>\n",
      "============================================================\n",
      "\n",
      "[Test 4] Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "------------------------------------------------------------\n",
      "Response:\n",
      "Natalia sold 48/2 = (48/2=24)24 clips in May.\n",
      "Natalia sold 48+24 = (48+24=72)72 clips altogether in April and May.\n",
      "</reasoning>\n",
      "<answer>\n",
      "72\n",
      "</answer>\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test questions\n",
    "test_questions = [\n",
    "    \"The present age of A is twice that of B. After 10 years, A will be 1.5 times B. What is A's present age?\",\n",
    "    \"If a shirt costs $25 and is on sale for 20% off, what is the sale price?\",\n",
    "    \"A train travels 60 miles in 45 minutes. What is its speed in miles per hour?\",\n",
    "    \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\",\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Testing Trained Model (Strict Format)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    # 1. Generate the formatted prompt\n",
    "    prompt = generate_inference_prompt(question)\n",
    "\n",
    "    print(f\"\\n[Test {i}] Question: {question}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # 2. Run Generation\n",
    "    sampler_output = generation_sampler(\n",
    "        input_strings=[prompt],\n",
    "        max_generation_steps=512,\n",
    "        temperature=0.01,  # Near-greedy for math\n",
    "        top_k=1,\n",
    "    )\n",
    "\n",
    "    # 3. Extract and Clean Response\n",
    "    response = sampler_output.text[0]\n",
    "    \n",
    "    # Manual Stop: Cut off text if the model generates <end_of_turn>\n",
    "    # This fixes the looping issue seen in Test 4\n",
    "    if \"<end_of_turn>\" in response:\n",
    "        response = response.split(\"<end_of_turn>\")[0]\n",
    "\n",
    "    print(f\"Response:\\n{response}\")\n",
    "    print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T03:02:52.751602Z",
     "iopub.status.busy": "2025-12-21T03:02:52.751264Z",
     "iopub.status.idle": "2025-12-21T03:02:52.894959Z",
     "shell.execute_reply": "2025-12-21T03:02:52.893727Z",
     "shell.execute_reply.started": "2025-12-21T03:02:52.751576Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 21: Evaluation loop with (optional) self-consistency voting\n",
    "\n",
    "Implements a more robust evaluation strategy:\n",
    "\n",
    "- Generate `VOTE_SAMPLES` completions per question (with `TEMPERATURE > 0`).\n",
    "- Parse each completion into a final answer candidate.\n",
    "- Use majority vote (`collections.Counter`) to pick the most frequent answer.\n",
    "- Compute final accuracy and log failures with candidate distributions.\n",
    "\n",
    "**Why self-consistency helps**\n",
    "Many math problems have multiple valid reasoning paths; sampling increases the chance you get at least one correct path, and voting reduces variance.\n",
    "\n",
    "**Tradeoff**\n",
    "Higher `VOTE_SAMPLES` improves accuracy but increases inference cost linearly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "VOTE_SAMPLES = 1 \n",
    "\n",
    "# Temperature must be > 0 to get diverse reasoning paths\n",
    "# 0.6 is standard for Self-Consistency\n",
    "TEMPERATURE = 0.7 \n",
    "\n",
    "# Max tokens for the answer\n",
    "MAX_GEN_STEPS = 512\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Evaluating with Majority Voting (k={VOTE_SAMPLES})\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "def normalize_answer(answer_str):\n",
    "    \"\"\"Normalize answer string for comparison.\"\"\"\n",
    "    if answer_str is None:\n",
    "        return None\n",
    "    s = str(answer_str).strip().lower()\n",
    "    s = s.replace('$', '').replace(',', '').replace('£', '').replace('€', '')\n",
    "    if s.endswith('.'):\n",
    "        s = s[:-1]\n",
    "    return s\n",
    "\n",
    "def extract_answer_robust(response):\n",
    "    \"\"\"\n",
    "    Extracts answers using a cascade of patterns (XML -> Boxed -> Text).\n",
    "    \"\"\"\n",
    "    # 1. Try <answer> tags\n",
    "    xml_match = re.search(r\"<answer>\\s*(.*?)\\s*</answer>\", response, re.DOTALL)\n",
    "    if xml_match:\n",
    "        return xml_match.group(1)\n",
    "\n",
    "    # 2. Try LaTeX \\boxed{}\n",
    "    boxed_match = re.search(r\"\\\\boxed\\{([^}]+)\\}\", response)\n",
    "    if boxed_match:\n",
    "        return boxed_match.group(1)\n",
    "\n",
    "    # 3. Try \"Final Answer\" text patterns\n",
    "    text_match = re.search(r\"(?:final answer|answer is)[:\\s]*([0-9\\.]+)\", response, re.IGNORECASE)\n",
    "    if text_match:\n",
    "        return text_match.group(1)\n",
    "\n",
    "    # 4. Fallback: Last number\n",
    "    numbers = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", response)\n",
    "    if numbers:\n",
    "        return numbers[-1]\n",
    "    return None\n",
    "\n",
    "def get_majority_vote(candidates):\n",
    "    \"\"\"Returns the most common answer from a list of candidates.\"\"\"\n",
    "    # Filter out None values\n",
    "    valid_candidates = [c for c in candidates if c is not None]\n",
    "    \n",
    "    if not valid_candidates:\n",
    "        return None\n",
    "    \n",
    "    # Count frequency\n",
    "    counter = collections.Counter(valid_candidates)\n",
    "    \n",
    "    # Get the most common element ((value, count) tuple)\n",
    "    most_common, count = counter.most_common(1)[0]\n",
    "    return most_common\n",
    "\n",
    "\n",
    "# Load dataset if not already loaded\n",
    "if 'test_dataset' not in globals():\n",
    "    from datasets import load_dataset\n",
    "    test_dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
    "\n",
    "total_examples = len(test_dataset)\n",
    "correct_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "# Store failures for analysis\n",
    "failures = []\n",
    "\n",
    "for idx in tqdm(range(total_examples), desc=\"Voting\"):\n",
    "    example = test_dataset[idx]\n",
    "    question = example[\"question\"]\n",
    "    \n",
    "    # Get Ground Truth\n",
    "    ground_truth_raw = extract_hash_answer(example[\"answer\"])\n",
    "    ground_truth_norm = normalize_answer(ground_truth_raw)\n",
    "\n",
    "    # Prepare Prompt\n",
    "    prompt = generate_inference_prompt(question)\n",
    "    \n",
    "    # Create Batch: Replicate the prompt VOTE_SAMPLES times\n",
    "    # This sends 8 identical prompts to the model at once\n",
    "    batch_prompts = [prompt] * VOTE_SAMPLES\n",
    "\n",
    "    try:\n",
    "        # Generate samples in parallel\n",
    "        sampler_output = generation_sampler(\n",
    "            input_strings=batch_prompts,\n",
    "            max_generation_steps=MAX_GEN_STEPS,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_k=40, # Allow diversity for voting\n",
    "        )\n",
    "        \n",
    "        # Extract answers from all samples\n",
    "        candidates = []\n",
    "        for response_text in sampler_output.text:\n",
    "            # Cleanup stop tokens\n",
    "            if \"<end_of_turn>\" in response_text:\n",
    "                response_text = response_text.split(\"<end_of_turn>\")[0]\n",
    "            \n",
    "            # Extract\n",
    "            raw_ans = extract_answer_robust(response_text)\n",
    "            norm_ans = normalize_answer(raw_ans)\n",
    "            candidates.append(norm_ans)\n",
    "            \n",
    "        # Perform Majority Vote\n",
    "        final_prediction = get_majority_vote(candidates)\n",
    "        \n",
    "        # Check Correctness\n",
    "        is_correct = False\n",
    "        if final_prediction is not None and ground_truth_norm is not None:\n",
    "            try:\n",
    "                is_correct = float(final_prediction) == float(ground_truth_norm)\n",
    "            except ValueError:\n",
    "                is_correct = final_prediction == ground_truth_norm\n",
    "        \n",
    "        if is_correct:\n",
    "            correct_count += 1\n",
    "        else:\n",
    "            # Log failure for inspection\n",
    "            failures.append({\n",
    "                \"q\": question,\n",
    "                \"gt\": ground_truth_norm,\n",
    "                \"pred\": final_prediction,\n",
    "                \"candidates\": candidates\n",
    "            })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error on example {idx}: {e}\")\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MAJORITY VOTING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Time: {total_time:.1f}s ({total_time/total_examples:.2f}s per question)\")\n",
    "print(f\"Samples per Question: {VOTE_SAMPLES}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Final Accuracy: {correct_count}/{total_examples} ({100*correct_count/total_examples:.2f}%)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show a sample failure to see voting behavior\n",
    "if failures:\n",
    "    print(\"\\nSample Failure (Voting Analysis):\")\n",
    "    f = failures[0]\n",
    "    print(f\"Question: {f['q'][:100]}...\")\n",
    "    print(f\"Ground Truth: {f['gt']}\")\n",
    "    print(f\"Voted Prediction: {f['pred']}\")\n",
    "    print(f\"Vote Distribution: {f['candidates']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub import login, upload_folder\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "api = HfApi()\n",
    "hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "login(token=hf_token)\n",
    "\n",
    "upload_folder(\n",
    "    folder_path=\"outputs_sft_full\",\n",
    "    repo_id=\"liuxiaohua72/lxh_gemma3\",\n",
    "    repo_type=\"model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you\n",
    "\n",
    "Still, a lot of training is required on this model. This was a basic training strategy"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpuV5e8",
   "dataSources": [
    {
     "databundleVersionId": 14363498,
     "sourceId": 119261,
     "sourceType": "competition"
    },
    {
     "datasetId": 9010164,
     "sourceId": 14138938,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4054119,
     "sourceId": 7045423,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 3301,
     "modelInstanceId": 8318,
     "sourceId": 28785,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 3533,
     "modelInstanceId": 5171,
     "sourceId": 208042,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 222398,
     "modelInstanceId": 239467,
     "sourceId": 282742,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31194,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
