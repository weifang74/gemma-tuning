{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":119261,"databundleVersionId":14363498,"sourceType":"competition"},{"sourceId":14138938,"sourceType":"datasetVersion","datasetId":9010164},{"sourceId":7045423,"sourceType":"datasetVersion","datasetId":4054119},{"sourceId":28785,"sourceType":"modelInstanceVersion","modelInstanceId":8318,"modelId":3301},{"sourceId":208042,"sourceType":"modelInstanceVersion","modelInstanceId":5171,"modelId":3533},{"sourceId":282742,"sourceType":"modelInstanceVersion","modelInstanceId":239467,"modelId":222398}],"dockerImageVersionId":31194,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Cell 0: Install Tunix and dependencies\n\nInstalls **Tunix** (Google’s JAX/TPU-first training and serving utilities) with the `prod` extras.  \nThis notebook assumes a Kaggle TPU runtime; installing inside the notebook guarantees the exact version (`0.1.3`) used when the notebook was authored.\n\n**Notes**\n- If you see dependency conflicts, restart the kernel after installation.\n- Pinning the version helps reproducibility across Kaggle sessions.\n","metadata":{}},{"cell_type":"code","source":"!pip install \"google-tunix[prod]==0.1.3\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-21T02:23:33.425318Z","iopub.execute_input":"2025-12-21T02:23:33.425968Z","iopub.status.idle":"2025-12-21T02:23:46.545550Z","shell.execute_reply.started":"2025-12-21T02:23:33.425945Z","shell.execute_reply":"2025-12-21T02:23:46.544477Z"},"_kg_hide-output":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: google-tunix==0.1.3 in /usr/local/lib/python3.12/site-packages (from google-tunix[prod]==0.1.3) (0.1.3)\nRequirement already satisfied: datasets in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.4.1)\nRequirement already satisfied: flax>=0.11.1 in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.12.0)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2025.10.0)\nRequirement already satisfied: gcsfs in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2025.10.0)\nRequirement already satisfied: grain in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.2.14)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.36.0)\nRequirement already satisfied: jaxtyping in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.1.6)\nRequirement already satisfied: kagglehub in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.3.13)\nRequirement already satisfied: numba in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.63.0b1)\nRequirement already satisfied: omegaconf in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.4.0.dev4)\nRequirement already satisfied: pylatexenc in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.0a33)\nRequirement already satisfied: python-dotenv in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.2.1)\nRequirement already satisfied: qwix in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.2)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.2.1)\nRequirement already satisfied: sympy in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.14.0)\nRequirement already satisfied: tensorboardX in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.6.4)\nRequirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.9.9)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.67.1)\nRequirement already satisfied: transformers in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.57.1)\nRequirement already satisfied: hf_transfer in /usr/local/lib/python3.12/site-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.9)\nRequirement already satisfied: jax!=0.7.2,>=0.6.0 in /usr/local/lib/python3.12/site-packages (from jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (0.8.0)\nRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.12/site-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.3.4)\nRequirement already satisfied: msgpack in /usr/local/lib/python3.12/site-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.1.2)\nRequirement already satisfied: optax in /usr/local/lib/python3.12/site-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.2.6)\nRequirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.12/site-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.11.27)\nRequirement already satisfied: tensorstore in /usr/local/lib/python3.12/site-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.78)\nRequirement already satisfied: rich>=11.1 in /usr/local/lib/python3.12/site-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (14.2.0)\nRequirement already satisfied: typing_extensions>=4.2 in /usr/local/lib/python3.12/site-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.15.0)\nRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.12/site-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (6.0.3)\nRequirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.12/site-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.10)\nRequirement already satisfied: jaxlib<=0.8.0,>=0.8.0 in /usr/local/lib/python3.12/site-packages (from jax!=0.7.2,>=0.6.0->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (0.8.0)\nRequirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/site-packages (from jax!=0.7.2,>=0.6.0->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (0.5.3)\nRequirement already satisfied: opt_einsum in /usr/local/lib/python3.12/site-packages (from jax!=0.7.2,>=0.6.0->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (3.4.0)\nRequirement already satisfied: scipy>=1.13 in /usr/local/lib/python3.12/site-packages (from jax!=0.7.2,>=0.6.0->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (1.16.3)\nCollecting libtpu==0.0.24.* (from jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3)\n  Downloading libtpu-0.0.24-cp312-cp312-manylinux_2_31_x86_64.whl.metadata (1.2 kB)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/site-packages (from jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (2.32.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/site-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.20.0)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/site-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (22.0.0)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/site-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/site-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.3.3)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/site-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.28.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.12/site-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/site-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.70.18)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/site-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (25.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/site-packages (from huggingface_hub->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.2.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/site-packages (from gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.13.2)\nRequirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.12/site-packages (from gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (5.2.1)\nRequirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.12/site-packages (from gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.42.1)\nRequirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.12/site-packages (from gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.2.2)\nRequirement already satisfied: google-cloud-storage in /usr/local/lib/python3.12/site-packages (from gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.5.0)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.12/site-packages (from grain->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.3.1)\nRequirement already satisfied: array-record>=0.8.1 in /usr/local/lib/python3.12/site-packages (from grain->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.8.2)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.12/site-packages (from grain->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.1.2)\nRequirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.12/site-packages (from grain->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.13.0)\nRequirement already satisfied: protobuf>=5.28.3 in /usr/local/lib/python3.12/site-packages (from grain->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (6.33.0)\nRequirement already satisfied: wadler-lindig>=0.1.3 in /usr/local/lib/python3.12/site-packages (from jaxtyping->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.7)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/site-packages (from jinja2->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.0.3)\nRequirement already satisfied: llvmlite<0.47,>=0.46.0dev0 in /usr/local/lib/python3.12/site-packages (from numba->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.46.0b1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/site-packages (from sympy->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.3.0)\nRequirement already satisfied: dm-tree in /usr/local/lib/python3.12/site-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.9)\nRequirement already satisfied: immutabledict in /usr/local/lib/python3.12/site-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.2.2)\nRequirement already satisfied: promise in /usr/local/lib/python3.12/site-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.3)\nRequirement already satisfied: psutil in /usr/local/lib/python3.12/site-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (7.1.3)\nRequirement already satisfied: simple_parsing in /usr/local/lib/python3.12/site-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.7)\nRequirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.12/site-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.17.2)\nRequirement already satisfied: termcolor in /usr/local/lib/python3.12/site-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.2.0)\nRequirement already satisfied: toml in /usr/local/lib/python3.12/site-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.10.2)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.12/site-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.0.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/site-packages (from transformers->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2025.11.3)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/site-packages (from transformers->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.22.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/site-packages (from transformers->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.7.0rc0)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.22.0)\nRequirement already satisfied: einops in /usr/local/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.8.1)\nRequirement already satisfied: importlib_resources in /usr/local/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (6.5.2)\nRequirement already satisfied: zipp in /usr/local/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.23.0)\nRequirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/site-packages (from google-auth>=1.2->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (6.2.1)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/site-packages (from google-auth>=1.2->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.4.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/site-packages (from google-auth>=1.2->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.9.1)\nRequirement already satisfied: anyio in /usr/local/lib/python3.12/site-packages (from httpx<1.0.0->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.11.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.12/site-packages (from httpx<1.0.0->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2025.10.5)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/site-packages (from httpx<1.0.0->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.12/site-packages (from httpx<1.0.0->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.16.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (2.5.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/site-packages (from rich>=11.1->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/site-packages (from rich>=11.1->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.19.2)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.12/site-packages (from google-auth-oauthlib->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.0.0)\nRequirement already satisfied: google-api-core<3.0.0,>=2.27.0 in /usr/local/lib/python3.12/site-packages (from google-cloud-storage->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.28.1)\nRequirement already satisfied: google-cloud-core<3.0.0,>=2.4.2 in /usr/local/lib/python3.12/site-packages (from google-cloud-storage->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.5.0)\nRequirement already satisfied: google-resumable-media<3.0.0,>=2.7.2 in /usr/local/lib/python3.12/site-packages (from google-cloud-storage->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.7.2)\nRequirement already satisfied: google-crc32c<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/site-packages (from google-cloud-storage->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.7.1)\nRequirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.12/site-packages (from optax->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.91)\nRequirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/site-packages (from orbax-checkpoint->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.6.0)\nRequirement already satisfied: aiofiles in /usr/local/lib/python3.12/site-packages (from orbax-checkpoint->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (25.1.0)\nRequirement already satisfied: humanize in /usr/local/lib/python3.12/site-packages (from orbax-checkpoint->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.14.0)\nRequirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.12/site-packages (from orbax-checkpoint->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.20.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/site-packages (from pandas->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/site-packages (from pandas->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/site-packages (from pandas->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2025.2)\nRequirement already satisfied: six in /usr/local/lib/python3.12/site-packages (from promise->tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.17.0)\nRequirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.12/site-packages (from simple_parsing->tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.17.0)\nRequirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /usr/local/lib/python3.12/site-packages (from tensorflow-metadata->tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.71.0)\nRequirement already satisfied: toolz>=1.0.0 in /usr/local/lib/python3.12/site-packages (from chex>=0.1.87->optax->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.1.0)\nRequirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/site-packages (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.26.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.2)\nRequirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.6.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.3.1)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/site-packages (from anyio->httpx<1.0.0->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.3.1)\nDownloading libtpu-0.0.24-cp312-cp312-manylinux_2_31_x86_64.whl (156.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.4/156.4 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: libtpu\n  Attempting uninstall: libtpu\n    Found existing installation: libtpu 0.0.17\n    Uninstalling libtpu-0.0.17:\n      Successfully uninstalled libtpu-0.0.17\nSuccessfully installed libtpu-0.0.24\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Cell 1: TPU/JAX runtime sanity checks + environment flags\n\n1. Imports JAX and prints a quick **device inventory** (backend, device kind, device list).  \n2. Warns if you are not on TPU (important because Gemma 3 training is intended to run on TPU in this notebook).  \n3. Sets several environment variables and JAX configs:\n   - `XLA_FLAGS` and `LIBTPU_INIT_ARGS`: performance and async collective behavior.\n   - `JAX_COMPILATION_CACHE_DIR`: speeds up repeated compiles.\n   - `jax_enable_x64=False`: keeps computation in 32-bit (typically BF16/FP32 mix) for speed/memory.\n   - `jax_default_matmul_precision='high'`: improves numerical stability for matmuls.\n\n**Pitfall**\n- If `jax.default_backend()` is not `tpu`, training will be extremely slow and results will not match the intended setup.\n","metadata":{}},{"cell_type":"code","source":"!pip list |grep proto","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T02:23:46.546365Z","iopub.execute_input":"2025-12-21T02:23:46.546550Z","iopub.status.idle":"2025-12-21T02:23:47.762861Z","shell.execute_reply.started":"2025-12-21T02:23:46.546531Z","shell.execute_reply":"2025-12-21T02:23:47.761625Z"}},"outputs":[{"name":"stdout","text":"googleapis-common-protos     1.71.0\nproto-plus                   1.26.1\nprotobuf                     6.33.0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!tensorboard outputs_sft_full/tensorboard\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T02:23:47.763548Z","iopub.execute_input":"2025-12-21T02:23:47.763735Z","iopub.status.idle":"2025-12-21T02:24:12.962920Z","shell.execute_reply.started":"2025-12-21T02:23:47.763717Z","shell.execute_reply":"2025-12-21T02:24:12.961865Z"}},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.12/site-packages/tensorboard/default.py:30: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  import pkg_resources\n/usr/local/lib/python3.12/site-packages/jax/_src/cloud_tpu_init.py:93: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n  warnings.warn(\nusage: tensorboard [-h] [--helpfull] {serve} ...\ntensorboard: error: argument {serve}: invalid choice: 'outputs_sft_full/tensorboard' (choose from serve)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import jax\nimport jax.numpy as jnp\nimport os\nimport warnings; \nwarnings.filterwarnings('ignore')\n\nprint(f\"JAX version: {jax.__version__}\")\nprint(f\"Number of devices: {len(jax.devices())}\")\nprint(f\"Device kind: {jax.devices()[0].device_kind}\")\nprint(f\"JAX backend: {jax.default_backend()}\")\nprint(f\"\\nDevices:\")\nfor i, device in enumerate(jax.devices()):\n    print(f\"  [{i}] {device}\")\nprint(\"=\"*60)\n\nif jax.default_backend() != 'tpu':\n    print(\"\\n⚠️  WARNING: Not running on TPU!\")\n    print(f\"   Current backend: {jax.default_backend()}\")\n    print(\"   Make sure you've selected TPU runtime in Kaggle\")\nelse:\n    print(\"\\n✓ TPU backend confirmed\")\n\n\nos.environ['XLA_FLAGS'] = (\n    '--xla_gpu_enable_triton_softmax_fusion=true '\n    '--xla_gpu_triton_gemm_any=True '\n    '--xla_gpu_enable_async_collectives=true'\n)\nos.environ['JAX_COMPILATION_CACHE_DIR'] = '/tmp/jax_cache'\nos.environ['LIBTPU_INIT_ARGS'] = '--xla_enable_async_all_gather=true'\n\njax.config.update('jax_enable_x64', False)  # Use 32-bit for speed\njax.config.update('jax_default_matmul_precision', 'high')  # BF16 matmuls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T02:24:12.963830Z","iopub.execute_input":"2025-12-21T02:24:12.964013Z","iopub.status.idle":"2025-12-21T02:24:22.252599Z","shell.execute_reply.started":"2025-12-21T02:24:12.963995Z","shell.execute_reply":"2025-12-21T02:24:22.251519Z"}},"outputs":[{"name":"stdout","text":"JAX version: 0.8.0\n","output_type":"stream"},{"name":"stderr","text":"WARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1766283854.283710      12 common_lib.cc:648] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: === \nlearning/45eac/tfrc/runtime/common_lib.cc:238\n","output_type":"stream"},{"name":"stdout","text":"Number of devices: 8\nDevice kind: TPU v5 lite\nJAX backend: tpu\n\nDevices:\n  [0] TPU_0(process=0,(0,0,0,0))\n  [1] TPU_1(process=0,(1,0,0,0))\n  [2] TPU_2(process=0,(0,1,0,0))\n  [3] TPU_3(process=0,(1,1,0,0))\n  [4] TPU_4(process=0,(0,2,0,0))\n  [5] TPU_5(process=0,(1,2,0,0))\n  [6] TPU_6(process=0,(0,3,0,0))\n  [7] TPU_7(process=0,(1,3,0,0))\n============================================================\n\n✓ TPU backend confirmed\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Cell 2: Experiment configuration (model, batching, training hyperparameters, output paths)\n\nDefines the main knobs for fine-tuning:\n\n- **Model handle** (`KAGGLE_MODEL_HANDLE`): points to Gemma 3 weights hosted on Kaggle.\n- **Sequence length** (`MAX_SEQ_LENGTH`): max tokens per example; impacts memory and speed.\n- **TPU mesh** (`MESH_SHAPE`): logical device mesh for sharding (FSDP axis and tensor-parallel axis).\n- **Micro-batch size** + **gradient accumulation**: together determine the **effective global batch size**.\n- **Optimizer hyperparams**: learning rate, warmup, weight decay, grad clipping, epochs/steps.\n- **Checkpoint/TensorBoard dirs** and logging cadence.\n\nThe printed “Global Batch Size” helps confirm your true effective batch:\n`micro_batch * num_devices * grad_accumulation`.\n\n**Note**\n- The `...` line in this cell is a placeholder in the original notebook source. If you run the notebook as-is, ensure all required constants (e.g., Adam betas/epsilon if referenced later) are defined somewhere.\n","metadata":{}},{"cell_type":"code","source":"KAGGLE_MODEL_HANDLE = \"google/gemma-3/transformers/gemma-3-1b-it\"\n\nMAX_SEQ_LENGTH = 2048\nMESH_SHAPE = (8, 1) \nTRAIN_MICRO_BATCH_SIZE = 2 \n\nGRADIENT_ACCUMULATION_STEPS = 4 \n\nLEARNING_RATE = 2e-5 \nWARMUP_STEPS = 50    \nNUM_EPOCHS = 3       \n\n\nMAX_STEPS = 200 * NUM_EPOCHS \n\n\nADAM_BETA1 = 0.9\n\nADAM_BETA2 = 0.999 \n\nADAM_EPSILON = 1e-8\n\n\nWEIGHT_DECAY = 0.01 \nMAX_GRAD_NORM = 1.0\n\nprint(f\"Global Batch Size: {TRAIN_MICRO_BATCH_SIZE * 8 * GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"Total Training Steps: {MAX_STEPS}\")\n\n\nCHECKPOINT_DIR = \"/kaggle/working/outputs_sft_full/checkpoints\"\nTENSORBOARD_DIR = \"/kaggle/working/outputs_sft_full/tensorboard\"\nSAVE_INTERVAL_STEPS = 100\nEVAL_INTERVAL_STEPS = 50\nLOG_INTERVAL_STEPS = 10\n\nprint(\"✓ Configuration loaded\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T02:49:16.419602Z","iopub.execute_input":"2025-12-21T02:49:16.419836Z","iopub.status.idle":"2025-12-21T02:49:16.424941Z","shell.execute_reply.started":"2025-12-21T02:49:16.419819Z","shell.execute_reply":"2025-12-21T02:49:16.424066Z"}},"outputs":[{"name":"stdout","text":"Global Batch Size: 64\nTotal Training Steps: 3000\n✓ Configuration loaded\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"## Cell 3: Download Gemma 3 from Kaggle and create a TPU device mesh\n\n- Uses `kagglehub.model_download()` to fetch the model assets locally.\n- Builds a JAX mesh (`jax.make_mesh`) with axes `('fsdp', 'tp')` using `MESH_SHAPE`.\n\nThis mesh is later used to:\n- **Shard parameters** across devices (FSDP-style parameter sharding).\n- Optionally use a tensor-parallel axis (depending on model/implementation).\n\n**Why this matters**\nWithout a mesh context, the model can silently remain on CPU, making training incorrect/slow.\n","metadata":{}},{"cell_type":"code","source":"import kagglehub\nfrom tunix.models.gemma3 import model as gemma_lib\nfrom tunix.models.gemma3 import params_safetensors as params_safetensors_lib\nfrom tunix.generate import tokenizer_adapter as tokenizer_lib\n\nprint(f\"Model handle: {KAGGLE_MODEL_HANDLE}\")\n\nlocal_model_path = kagglehub.model_download(KAGGLE_MODEL_HANDLE)\nprint(f\"✓ Model downloaded to: {local_model_path}\")\n\nprint(f\"\\nCreating TPU mesh with shape {MESH_SHAPE}...\")\nmesh = jax.make_mesh(MESH_SHAPE, ('fsdp', 'tp'))\nprint(f\"✓ TPU Mesh created successfully\")\nprint(f\"  Mesh shape: {mesh.shape}\")\nprint(f\"  Mesh axis names: {mesh.axis_names}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T02:49:21.499408Z","iopub.execute_input":"2025-12-21T02:49:21.499658Z","iopub.status.idle":"2025-12-21T02:49:21.925357Z","shell.execute_reply.started":"2025-12-21T02:49:21.499640Z","shell.execute_reply":"2025-12-21T02:49:21.924330Z"}},"outputs":[{"name":"stdout","text":"Model handle: google/gemma-3/transformers/gemma-3-1b-it\n✓ Model downloaded to: /kaggle/input/gemma-3/transformers/gemma-3-1b-it/1\n\nCreating TPU mesh with shape (8, 1)...\n✓ TPU Mesh created successfully\n  Mesh shape: OrderedDict({'fsdp': 8, 'tp': 1})\n  Mesh axis names: ('fsdp', 'tp')\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"## Cell 4: Load model weights (.safetensors) and tokenizer\n\n- Creates a Gemma 3 1B model config.\n- Loads model parameters from the downloaded `.safetensors` files into a JAX/Flax model, sharded according to the TPU mesh.\n- Loads the SentencePiece tokenizer (`tokenizer.model`) matching the base checkpoint.\n\n**Key idea**\nTokenizer and model weights must match; mixing tokenizers across checkpoints can corrupt training and evaluation.\n","metadata":{}},{"cell_type":"code","source":"model_config = gemma_lib.ModelConfig.gemma3_1b()\n\ngemma3_model = params_safetensors_lib.create_model_from_safe_tensors(\n    local_model_path,  # Directory containing .safetensors files\n    model_config,\n    mesh,\n)\nprint(\"✓ Model loaded successfully\")\n\n\ntokenizer = tokenizer_lib.Tokenizer(\n    tokenizer_path=f\"{local_model_path}/tokenizer.model\"\n)\nprint(\"✓ Tokenizer loaded successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T02:49:25.813844Z","iopub.execute_input":"2025-12-21T02:49:25.814105Z","iopub.status.idle":"2025-12-21T02:49:27.205929Z","shell.execute_reply.started":"2025-12-21T02:49:25.814084Z","shell.execute_reply":"2025-12-21T02:49:27.204905Z"}},"outputs":[{"name":"stdout","text":"✓ Model loaded successfully\n✓ Tokenizer loaded successfully\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"## Cell 5: Force model parameter sharding onto TPU and verify placement\n\n- Uses `flax.nnx` utilities to:\n  - extract model state (`nnx.state`)\n  - compute partition specs (`nnx.get_partition_spec`)\n  - apply sharding constraints (`jax.lax.with_sharding_constraint`)\n  - update the model with the sharded state (`nnx.update`)\n- “Materializes” shapes to force device placement.\n- Then inspects a sample parameter to confirm it resides on TPU devices.\n\n**Why this exists**\nIn JAX it is possible to construct objects on host/CPU and only later place them on device. This explicit sharding/verification prevents a common failure mode: “training runs but on CPU”.\n\n**Note about `...`**\nThe `...` line is a placeholder from the original notebook and is not executable Python. If this notebook errors at runtime, remove/replace those placeholders.\n","metadata":{}},{"cell_type":"code","source":"import flax.nnx as nnx\n\n\nmodel_input = gemma3_model.get_model_input()\n\nprint(\"\\nSharding model across TPU devices...\")\nwith mesh:\n    state = nnx.state(gemma3_model)\n    pspecs = nnx.get_partition_spec(state)\n    sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n    nnx.update(gemma3_model, sharded_state)\n    \n    # Force materialization on TPU\n    _ = jax.tree_util.tree_map(lambda x: x.shape if hasattr(x, 'shape') else x, state)\n    \n\n\ntotal_params = sum(p.size for p in jax.tree_util.tree_leaves(nnx.state(gemma3_model)))\n\nprint(f\"\\n✓ Model ready for full fine-tuning\")\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"Trainable parameters: {total_params:,}\")\n\n\nall_params = nnx.state(gemma3_model)\nparam_leaves = jax.tree_util.tree_leaves(all_params)\nprint(f\"Number of parameters: {len(param_leaves)}\")\n\nif len(param_leaves) > 0:\n    sample = param_leaves[0]\n    print(f\"Sample param shape: {sample.shape}\")\n    print(f\"Sample param dtype: {sample.dtype}\")\n    \n    # Check device placement\n    if hasattr(sample, 'devices'):\n        devices_set = sample.devices()\n        print(f\"Sample param devices: {list(devices_set)}\")\n        if len(devices_set) > 0:\n            dev = list(devices_set)[0]\n            device_kind = dev.device_kind\n            print(f\"Device kind: {device_kind}\")\n            if 'tpu' in device_kind.lower():\n                print(\"✓✓✓ SUCCESS: Model parameters are on TPU!\")\n                print(f\"✓✓✓ Confirmed: {device_kind} detected\")\n            else:\n                print(f\"❌❌❌ ERROR: Model parameters are on {device_kind}, NOT TPU!\")\n                print(\"Training will run on CPU and produce wrong results!\")\n    else:\n        print(\"⚠️  Cannot determine device placement\")\nelse:\n    print(\"❌ NO parameters found!\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T02:49:28.786531Z","iopub.execute_input":"2025-12-21T02:49:28.786813Z","iopub.status.idle":"2025-12-21T02:49:29.033653Z","shell.execute_reply.started":"2025-12-21T02:49:28.786794Z","shell.execute_reply":"2025-12-21T02:49:29.032603Z"}},"outputs":[{"name":"stdout","text":"\nSharding model across TPU devices...\n\n✓ Model ready for full fine-tuning\nTotal parameters: 999,885,952\nTrainable parameters: 999,885,952\nNumber of parameters: 314\nSample param shape: (262144, 1152)\nSample param dtype: bfloat16\nSample param devices: [TpuDevice(id=7, process_index=0, coords=(1,3,0), core_on_chip=0), TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=6, process_index=0, coords=(0,3,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(1,2,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=4, process_index=0, coords=(0,2,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)]\nDevice kind: TPU v5 lite\n✓✓✓ SUCCESS: Model parameters are on TPU!\n✓✓✓ Confirmed: TPU v5 lite detected\n============================================================\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"## Cell 6: Build an inference sampler (generation) + prompt constructor\n\n- Configures the KV cache (`CacheConfig`) for autoregressive generation.\n- Instantiates `sampler_lib.Sampler` with the model and tokenizer.\n- Defines `generate_inference_prompt(question)` which formats the input exactly like training:\n  - `<start_of_turn>user` + system instructions + question\n  - `<start_of_turn>model` + opens `<reasoning>` tag (the model is expected to continue)\n\n**Why it matters**\nEvaluation should mirror training formatting to get an apples-to-apples baseline and post-training comparison.\n","metadata":{}},{"cell_type":"code","source":"from tunix.generate import sampler as sampler_lib\nimport json\nimport os\n\n\ncache_config = sampler_lib.CacheConfig(\n    cache_size=MAX_SEQ_LENGTH + 512,\n    num_layers=model_config.num_layers,\n    num_kv_heads=model_config.num_kv_heads,\n    head_dim=model_config.head_dim,\n)\n\n\ngeneration_sampler = sampler_lib.Sampler(\n    transformer=gemma3_model,\n    tokenizer=tokenizer,\n    cache_config=cache_config,\n)\n\n\ndef generate_inference_prompt(question):\n    # Match the training exactly: Same System Prompt, No One-Shot needed anymore.\n    text = f\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n\\n{question}<end_of_turn>\\n\"\n    text += f\"<start_of_turn>model\\n<reasoning>\\n\" \n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T02:49:45.139718Z","iopub.execute_input":"2025-12-21T02:49:45.139953Z","iopub.status.idle":"2025-12-21T02:49:45.536232Z","shell.execute_reply.started":"2025-12-21T02:49:45.139936Z","shell.execute_reply":"2025-12-21T02:49:45.535129Z"}},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"## Cell 7: Define the strict instruction format and templates\n\nSets up:\n- XML-style tags used in training/eval:\n  - `<reasoning>...</reasoning>`\n  - `<answer>...</answer>`\n- A **SYSTEM_PROMPT** that forces the model to follow the schema.\n- Prompt templates showing how a full supervised example is constructed.\n\n**Goal**\nThis is “schema SFT”: you teach the model not just to solve problems, but to consistently produce machine-parseable outputs.\n\n**Note**\n`PROMPT_TEMPLATE` contains a `...` placeholder in the notebook. Replace it with a concrete template if you intend to use it directly.\n","metadata":{}},{"cell_type":"code","source":"import re\nfrom datasets import load_dataset\nreasoning_start = \"<reasoning>\"\nreasoning_end = \"</reasoning>\"\nsolution_start = \"<answer>\"\nsolution_end = \"</answer>\"\n\nSYSTEM_PROMPT = (\n    \"Solve the math problem. \"\n    \"You must STRICTLY follow this format:\\n\"\n    \"1. Enclose your step-by-step logic inside <reasoning>...</reasoning> tags.\\n\"\n    \"2. Enclose the final numerical result inside <answer>...</answer> tags.\"\n)\n\n\nPROMPT_TEMPLATE = \"\"\"<start_of_turn>user\n{system_instruction}\n\n{question}<end_of_turn>\n<start_of_turn>model\n\"\"\"\n\n\nFULL_TEMPLATE = \"\"\"<start_of_turn>user\n{system_prompt}\n\n{question}<end_of_turn>\n<start_of_turn>model\n\n{reasoning_start}\n{reasoning}\n{reasoning_end}\n\n{solution_start}\n{answer}\n{solution_end}<end_of_turn>\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T02:49:49.259586Z","iopub.execute_input":"2025-12-21T02:49:49.259880Z","iopub.status.idle":"2025-12-21T02:49:49.263701Z","shell.execute_reply.started":"2025-12-21T02:49:49.259861Z","shell.execute_reply":"2025-12-21T02:49:49.262619Z"}},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":"## Cell 8: Load evaluation questions + define answer extraction and scoring\n\n- Attempts to load a CSV of questions and gold answers (`updated_200_math_questions.csv`).\n  - Falls back to a hard-coded list if the CSV is missing.\n- Defines utilities to:\n  - extract a final answer from the model output (prefer `<answer>`, then GSM8K `####`, then last numeric token)\n  - normalize answers (strip commas/currency, normalize whitespace/case)\n  - compare predictions to gold answers, including handling cases like `\"x or y\"`.\n\n**Why this is important**\nLLM outputs are messy. Robust evaluation requires:\n1) deterministic parsing rules, and  \n2) normalization to avoid false negatives from formatting differences.\n","metadata":{}},{"cell_type":"code","source":"import re\nimport pandas as pd\n\n# -----------------------------\n# 1) Load questions\n# -----------------------------\n# Option A: Evaluate from CSV (recommended for your 200 questions)\nCSV_PATH = \"/kaggle/input/maths-sft-training-dataset/updated_200_math_questions.csv\"   # adjust if needed\n\ntry:\n    dfq = pd.read_csv(CSV_PATH)\n    questions = dfq[\"question\"].tolist()\n    golds = dfq[\"gold_answer\"].astype(str).tolist()\n    source = f\"CSV: {CSV_PATH} ({len(dfq)} rows)\"\nexcept Exception as e:\n    # Option B: fallback to your manual list\n    test_questions = [\n        \"What is the square root of 144?\",\n        \"If a shirt costs $25 and is on sale for 20% off, what is the sale price?\",\n        \"A train travels 60 miles in 45 minutes. What is its speed in miles per hour?\",\n        \"What is 15% of 200?\",\n        \"A product is marked up by 25% and then discounted by 20%. The final price is ₹960. What was the original price?\",\n        \"A car travels at 60 km/h for 30 minutes, stops for 10 minutes, then travels at 40 km/h for another 30 minutes. What is the car’s average speed for the entire journey?\",\n        \"What is ⅔ of ¾ of 120, minus 25% of the result?\",\n        \"The ratio of apples to oranges in a basket is 3:5. If 16 oranges are removed and the new ratio becomes 3:1,how many apples were originally in the basket?\",\n        \"A pipe fills a tank in 40 minutes, while another pipe empties the same tank in 60 minutes. If both pipes are opened together, how long will it take to fill the tank?\",\n        \"A number increases by 10% and then decreases by 10%. Is the final number greater than, less than, or equal to the original? Explain why.\",\n    ]\n    questions = test_questions\n    golds = [None] * len(questions)  # no golds in this path\n    source = f\"Manual list ({len(questions)} questions)\"\n    print(\"CSV load failed, using manual list. Error:\", e)\n\nprint(\"Evaluating:\", source)\n\n\n# -----------------------------\n# 2) Helpers: normalize + extract answers\n# -----------------------------\ndef normalize_text(s: str) -> str:\n    s = str(s).strip().lower()\n    # normalize unicode fractions (⅔ etc.) if they appear in answers (rare)\n    s = (s.replace(\"½\", \"1/2\")\n           .replace(\"⅓\", \"1/3\").replace(\"⅔\", \"2/3\")\n           .replace(\"¼\", \"1/4\").replace(\"¾\", \"3/4\")\n           .replace(\"⅕\", \"1/5\").replace(\"⅖\", \"2/5\").replace(\"⅗\", \"3/5\").replace(\"⅘\", \"4/5\")\n           .replace(\"⅙\", \"1/6\").replace(\"⅚\", \"5/6\")\n           .replace(\"⅛\", \"1/8\").replace(\"⅜\", \"3/8\").replace(\"⅝\", \"5/8\").replace(\"⅞\", \"7/8\"))\n    # remove spaces around common separators\n    s = re.sub(r\"\\s+\", \" \", s)\n    # remove currency symbols but keep numbers/units\n    s = s.replace(\"₹\", \"\").replace(\"$\", \"\")\n    # remove commas in numbers: 62,500 -> 62500\n    s = re.sub(r\"(?<=\\d),(?=\\d)\", \"\", s)\n    # trim punctuation at ends\n    s = s.strip(\" .,:;!?\\n\\t\")\n    return s\n\ndef extract_final_from_response(response: str) -> str:\n    \"\"\"\n    Tries in this order:\n    1) <answer>...</answer>\n    2) line starting with #### (gsm8k)\n    3) last numeric/fraction token in response\n    4) fallback: last non-empty line\n    \"\"\"\n    if response is None:\n        return \"\"\n\n    text = str(response)\n\n    # cut off runaway turns if present\n    if \"<end_of_turn>\" in text:\n        text = text.split(\"<end_of_turn>\")[0]\n\n    # 1) <answer> tag\n    m = re.search(r\"<answer>\\s*(.*?)\\s*</answer>\", text, flags=re.DOTALL | re.IGNORECASE)\n    if m:\n        return m.group(1).strip()\n\n    # 2) GSM8K #### final\n    m = re.search(r\"####\\s*(.+)\", text)\n    if m:\n        return m.group(1).strip()\n\n    # 3) last fraction or number (keeps % too)\n    tokens = re.findall(r\"-?\\d+(?:\\.\\d+)?(?:/\\d+(?:\\.\\d+)?)?%?\", text)\n    if tokens:\n        return tokens[-1].strip()\n\n    # 4) fallback: last non-empty line\n    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n    return lines[-1] if lines else text.strip()\n\ndef gold_to_accept_set(gold: str):\n    \"\"\"\n    Handles cases like '3 or 8' by allowing multiple correct answers.\n    \"\"\"\n    if gold is None:\n        return set()\n\n    g = normalize_text(gold)\n\n    # allow 'x or y' answers\n    if \" or \" in g:\n        parts = [p.strip() for p in g.split(\" or \") if p.strip()]\n        return set(parts)\n\n    return {g}\n\ndef is_correct(model_final: str, gold: str) -> bool:\n    mf = normalize_text(model_final)\n    accept = gold_to_accept_set(gold)\n    if not accept:\n        return False  # if no gold, can't score\n    return mf in accept","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T02:50:06.386407Z","iopub.execute_input":"2025-12-21T02:50:06.386675Z","iopub.status.idle":"2025-12-21T02:50:06.401296Z","shell.execute_reply.started":"2025-12-21T02:50:06.386655Z","shell.execute_reply":"2025-12-21T02:50:06.400289Z"}},"outputs":[{"name":"stdout","text":"Evaluating: CSV: /kaggle/input/maths-sft-training-dataset/updated_200_math_questions.csv (200 rows)\n","output_type":"stream"}],"execution_count":44},{"cell_type":"markdown","source":"## Cell 9: Run baseline inference and log per-question results\n\nLoops over `(question, gold)` pairs and:\n- Builds the prompt with `generate_inference_prompt`.\n- Calls `generation_sampler` with near-deterministic decoding (`temperature=0.01`, `top_k=1`).\n- Extracts the final answer and checks correctness.\n- Stores a rich record per example:\n  - prompt, raw response, parsed answer, gold answer, correctness flag\n\nOutputs `df_res.sample(4)` for a quick spot check.\n\n**Tip**\nIf you want more diverse reasoning, raise temperature (but that makes scoring noisier unless you use voting/self-consistency).\n","metadata":{}},{"cell_type":"code","source":"%%time\n# -----------------------------\n# 3) Run evaluation\n# -----------------------------\nresults = []\n\nfor i, (q, gold) in enumerate(zip(questions, golds), 1):\n    prompt = generate_inference_prompt(q)\n\n    out = generation_sampler(\n        input_strings=[prompt],\n        max_generation_steps=256,\n        temperature=0.01,\n        top_k=1,\n    )\n\n    response_raw = out.text[0]\n    model_final = extract_final_from_response(response_raw)\n\n    correct = None\n    if gold is not None:\n        correct = is_correct(model_final, gold)\n\n    results.append({\n        \"idx\": i,\n        \"question\": q,\n        \"gold_answer\": gold,\n        \"prompt\": prompt,\n        \"model_final_answer\": model_final,\n        \"model_raw_response\": response_raw,\n        \"is_correct\": correct\n    })\n\ndf_res = pd.DataFrame(results)\ndf_res.sample(4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T02:50:11.429765Z","iopub.execute_input":"2025-12-21T02:50:11.430057Z","iopub.status.idle":"2025-12-21T02:51:51.064898Z","shell.execute_reply.started":"2025-12-21T02:50:11.430031Z","shell.execute_reply":"2025-12-21T02:51:51.064085Z"}},"outputs":[{"name":"stdout","text":"CPU times: user 48.1 s, sys: 12.4 s, total: 1min\nWall time: 1min 39s\n","output_type":"stream"},{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"     idx                                           question gold_answer  \\\n178  179  The population of a town increases by 5% annua...        8000   \n170  171            Evaluate: 12 - [6 - {4 - (8 - 6) + 3}].          11   \n129  130       The sum of interior angles of a pentagon is?        540°   \n177  178  Price of sugar rises by 20%. By how much perce...      16.67%   \n\n                                                prompt model_final_answer  \\\n178  <start_of_turn>user\\nSolve the math problem. Y...           0.860777   \n170  <start_of_turn>user\\nSolve the math problem. Y...                  0   \n129  <start_of_turn>user\\nSolve the math problem. Y...                180   \n177  <start_of_turn>user\\nSolve the math problem. Y...               1.20   \n\n                                    model_raw_response  is_correct  \n178  We are given that the current population of a ...       False  \n170  \\nLet's break down the expression step-by-step...       False  \n129  The sum of interior angles of a polygon is alw...       False  \n177  To find the percent reduction in consumption n...       False  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>idx</th>\n      <th>question</th>\n      <th>gold_answer</th>\n      <th>prompt</th>\n      <th>model_final_answer</th>\n      <th>model_raw_response</th>\n      <th>is_correct</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>178</th>\n      <td>179</td>\n      <td>The population of a town increases by 5% annua...</td>\n      <td>8000</td>\n      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n      <td>0.860777</td>\n      <td>We are given that the current population of a ...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>170</th>\n      <td>171</td>\n      <td>Evaluate: 12 - [6 - {4 - (8 - 6) + 3}].</td>\n      <td>11</td>\n      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n      <td>0</td>\n      <td>\\nLet's break down the expression step-by-step...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>129</th>\n      <td>130</td>\n      <td>The sum of interior angles of a pentagon is?</td>\n      <td>540°</td>\n      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n      <td>180</td>\n      <td>The sum of interior angles of a polygon is alw...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>177</th>\n      <td>178</td>\n      <td>Price of sugar rises by 20%. By how much perce...</td>\n      <td>16.67%</td>\n      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n      <td>1.20</td>\n      <td>To find the percent reduction in consumption n...</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"print(df_res.iloc[36].question)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T02:26:34.921361Z","iopub.execute_input":"2025-12-21T02:26:34.921546Z","iopub.status.idle":"2025-12-21T02:26:34.925245Z","shell.execute_reply.started":"2025-12-21T02:26:34.921528Z","shell.execute_reply":"2025-12-21T02:26:34.924319Z"}},"outputs":[{"name":"stdout","text":"The present age of A is twice that of B. After 10 years, A will be 1.5 times B. What is A's present age?\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## Cell 10: Summarize baseline accuracy and surface failures\n\nCreates:\n- a one-row summary table (total, correct, wrong, accuracy)\n- `wrong_df`: a failure report including `prompt` and full `model_raw_response`\n\n**Why this is useful**\nWhen doing SFT, the fastest quality loop is:\n1) inspect failure modes,  \n2) adjust formatting/training data,  \n3) re-train,  \n4) re-evaluate with the same harness.\n","metadata":{}},{"cell_type":"code","source":"%%time\n# -----------------------------\n# 4) Summary tables\n# -----------------------------\nif df_res[\"is_correct\"].notna().any():\n    total = df_res[\"is_correct\"].notna().sum()\n    correct_n = int((df_res[\"is_correct\"] == True).sum())\n    wrong_n = int((df_res[\"is_correct\"] == False).sum())\n    acc = correct_n / total if total else 0.0\n\n    summary = pd.DataFrame([{\n        \"total_scored\": total,\n        \"correct\": correct_n,\n        \"wrong\": wrong_n,\n        \"accuracy_%\": round(acc * 100, 2),\n    }])\n    display(summary)\n\n    wrong_df = df_res[df_res[\"is_correct\"] == False][\n        [\"idx\", \"question\", \"gold_answer\", \"model_final_answer\", \"prompt\", \"model_raw_response\"]\n    ].reset_index(drop=True)\n\n    display(wrong_df)\nelse:\n    print(\"No gold answers were loaded, so scoring is skipped.\")\n    display(df_res[[\"idx\", \"question\", \"prompt\", \"model_final_answer\", \"model_raw_response\"]])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T02:53:05.044819Z","iopub.execute_input":"2025-12-21T02:53:05.045057Z","iopub.status.idle":"2025-12-21T02:53:05.060492Z","shell.execute_reply.started":"2025-12-21T02:53:05.045038Z","shell.execute_reply":"2025-12-21T02:53:05.059703Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"   total_scored  correct  wrong  accuracy_%\n0           200       74    126        37.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>total_scored</th>\n      <th>correct</th>\n      <th>wrong</th>\n      <th>accuracy_%</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>200</td>\n      <td>74</td>\n      <td>126</td>\n      <td>37.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"     idx                                           question gold_answer  \\\n0      1  A number is increased by 20% to become 360. Wh...         300   \n1      5  The average of 5 consecutive numbers is 64. Wh...          62   \n2      6  The average age of 4 people is 30 years. When ...          40   \n3      7  The ratio of boys to girls is 5:3. If 10 girls...          50   \n4      8  The ratio of milk to water is 7:3. If 6 liters...   42 liters   \n..   ...                                                ...         ...   \n121  193                               Multiply: 0.2 x 0.3.        0.06   \n122  194                                 Divide: 1.5 / 0.5.           3   \n123  195                            Reciprocal of 2 1/3 is?         3/7   \n124  199  Greatest number that divides 43, 91 and 183 le...           4   \n125  200  Three bells ring at intervals of 10, 15, 20 mi...       11 AM   \n\n    model_final_answer                                             prompt  \\\n0                 1800  <start_of_turn>user\\nSolve the math problem. Y...   \n1                   64  <start_of_turn>user\\nSolve the math problem. Y...   \n2                    2  <start_of_turn>user\\nSolve the math problem. Y...   \n3                   25  <start_of_turn>user\\nSolve the math problem. Y...   \n4                    6  <start_of_turn>user\\nSolve the math problem. Y...   \n..                 ...                                                ...   \n121                0.6  <start_of_turn>user\\nSolve the math problem. Y...   \n122                3.0  <start_of_turn>user\\nSolve the math problem. Y...   \n123                  9  <start_of_turn>user\\nSolve the math problem. Y...   \n124                 43  <start_of_turn>user\\nSolve the math problem. Y...   \n125              26:10  <start_of_turn>user\\nSolve the math problem. Y...   \n\n                                    model_raw_response  \n0    We are given that a number is increased by 20%...  \n1    The problem states that the average of 5 conse...  \n2    Let the ages of the first four people be $a_1,...  \n3    Let's analyze the problem step-by-step.\\nIniti...  \n4    Let $m$ be the initial amount of milk and $w$ ...  \n..                                                 ...  \n121  \\nTo multiply 0.2 by 0.3, we can multiply the ...  \n122  \\nTo divide 1.5 by 0.5, we can rewrite the div...  \n123  \\nTo find the reciprocal of 2 1/3, we need to ...  \n124  To find the greatest number that divides 43, 9...  \n125  The bells ring at intervals of 10, 15, and 20 ...  \n\n[126 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>idx</th>\n      <th>question</th>\n      <th>gold_answer</th>\n      <th>model_final_answer</th>\n      <th>prompt</th>\n      <th>model_raw_response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>A number is increased by 20% to become 360. Wh...</td>\n      <td>300</td>\n      <td>1800</td>\n      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n      <td>We are given that a number is increased by 20%...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5</td>\n      <td>The average of 5 consecutive numbers is 64. Wh...</td>\n      <td>62</td>\n      <td>64</td>\n      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n      <td>The problem states that the average of 5 conse...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6</td>\n      <td>The average age of 4 people is 30 years. When ...</td>\n      <td>40</td>\n      <td>2</td>\n      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n      <td>Let the ages of the first four people be $a_1,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7</td>\n      <td>The ratio of boys to girls is 5:3. If 10 girls...</td>\n      <td>50</td>\n      <td>25</td>\n      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n      <td>Let's analyze the problem step-by-step.\\nIniti...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8</td>\n      <td>The ratio of milk to water is 7:3. If 6 liters...</td>\n      <td>42 liters</td>\n      <td>6</td>\n      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n      <td>Let $m$ be the initial amount of milk and $w$ ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>121</th>\n      <td>193</td>\n      <td>Multiply: 0.2 x 0.3.</td>\n      <td>0.06</td>\n      <td>0.6</td>\n      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n      <td>\\nTo multiply 0.2 by 0.3, we can multiply the ...</td>\n    </tr>\n    <tr>\n      <th>122</th>\n      <td>194</td>\n      <td>Divide: 1.5 / 0.5.</td>\n      <td>3</td>\n      <td>3.0</td>\n      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n      <td>\\nTo divide 1.5 by 0.5, we can rewrite the div...</td>\n    </tr>\n    <tr>\n      <th>123</th>\n      <td>195</td>\n      <td>Reciprocal of 2 1/3 is?</td>\n      <td>3/7</td>\n      <td>9</td>\n      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n      <td>\\nTo find the reciprocal of 2 1/3, we need to ...</td>\n    </tr>\n    <tr>\n      <th>124</th>\n      <td>199</td>\n      <td>Greatest number that divides 43, 91 and 183 le...</td>\n      <td>4</td>\n      <td>43</td>\n      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n      <td>To find the greatest number that divides 43, 9...</td>\n    </tr>\n    <tr>\n      <th>125</th>\n      <td>200</td>\n      <td>Three bells ring at intervals of 10, 15, 20 mi...</td>\n      <td>11 AM</td>\n      <td>26:10</td>\n      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n      <td>The bells ring at intervals of 10, 15, and 20 ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>126 rows × 6 columns</p>\n</div>"},"metadata":{}},{"name":"stdout","text":"CPU times: user 9.97 ms, sys: 917 μs, total: 10.9 ms\nWall time: 10.3 ms\n","output_type":"stream"}],"execution_count":47},{"cell_type":"markdown","source":"## Cell 11: Quick look at wrong predictions\n\nDisplays `wrong_df.head()` so you can immediately inspect the first few mistakes with:\n- the question\n- the expected answer\n- the model’s parsed final answer\n- the full prompt and raw completion\n","metadata":{}},{"cell_type":"code","source":"wrong_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T02:53:13.506896Z","iopub.execute_input":"2025-12-21T02:53:13.507229Z","iopub.status.idle":"2025-12-21T02:53:13.515147Z","shell.execute_reply.started":"2025-12-21T02:53:13.507203Z","shell.execute_reply":"2025-12-21T02:53:13.514265Z"}},"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"   idx                                           question gold_answer  \\\n0    1  A number is increased by 20% to become 360. Wh...         300   \n1    5  The average of 5 consecutive numbers is 64. Wh...          62   \n2    6  The average age of 4 people is 30 years. When ...          40   \n3    7  The ratio of boys to girls is 5:3. If 10 girls...          50   \n4    8  The ratio of milk to water is 7:3. If 6 liters...   42 liters   \n\n  model_final_answer                                             prompt  \\\n0               1800  <start_of_turn>user\\nSolve the math problem. Y...   \n1                 64  <start_of_turn>user\\nSolve the math problem. Y...   \n2                  2  <start_of_turn>user\\nSolve the math problem. Y...   \n3                 25  <start_of_turn>user\\nSolve the math problem. Y...   \n4                  6  <start_of_turn>user\\nSolve the math problem. Y...   \n\n                                  model_raw_response  \n0  We are given that a number is increased by 20%...  \n1  The problem states that the average of 5 conse...  \n2  Let the ages of the first four people be $a_1,...  \n3  Let's analyze the problem step-by-step.\\nIniti...  \n4  Let $m$ be the initial amount of milk and $w$ ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>idx</th>\n      <th>question</th>\n      <th>gold_answer</th>\n      <th>model_final_answer</th>\n      <th>prompt</th>\n      <th>model_raw_response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>A number is increased by 20% to become 360. Wh...</td>\n      <td>300</td>\n      <td>1800</td>\n      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n      <td>We are given that a number is increased by 20%...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5</td>\n      <td>The average of 5 consecutive numbers is 64. Wh...</td>\n      <td>62</td>\n      <td>64</td>\n      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n      <td>The problem states that the average of 5 conse...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6</td>\n      <td>The average age of 4 people is 30 years. When ...</td>\n      <td>40</td>\n      <td>2</td>\n      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n      <td>Let the ages of the first four people be $a_1,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7</td>\n      <td>The ratio of boys to girls is 5:3. If 10 girls...</td>\n      <td>50</td>\n      <td>25</td>\n      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n      <td>Let's analyze the problem step-by-step.\\nIniti...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8</td>\n      <td>The ratio of milk to water is 7:3. If 6 liters...</td>\n      <td>42 liters</td>\n      <td>6</td>\n      <td>&lt;start_of_turn&gt;user\\nSolve the math problem. Y...</td>\n      <td>Let $m$ be the initial amount of milk and $w$ ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":48},{"cell_type":"markdown","source":"# Pre- Fine tuning model process","metadata":{}},{"cell_type":"markdown","source":"## Cell 12: GSM8K answer extraction helper (`#### ...`)\n\nDefines a helper to extract the final numeric answer from GSM8K examples, which commonly use the pattern:\n\n`... #### 42`\n\n**Why it matters**\nYou need a reliable way to obtain the “gold” final answer so you can build supervised `<answer>...</answer>` targets for SFT.\n","metadata":{}},{"cell_type":"code","source":"# Helper function to extract answer from GSM8K format\ndef extract_hash_answer(text):\n    \"\"\"Extract numerical answer after #### delimiter.\"\"\"\n    if \"####\" not in text:\n        return None\n    return text.split(\"####\")[1].strip()\n\n# Helper function to extract reasoning from GSM8K format\ndef extract_reasoning(text):\n    \"\"\"Extract reasoning (everything before #### delimiter).\"\"\"\n    if \"####\" not in text:\n        return text.strip()\n    return text.split(\"####\")[0].strip()\n\n# Load GSM8K dataset\nprint(\"Loading GSM8K dataset...\")\ntrain_dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\")\ntest_dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\nprint(f\"✓ Loaded {len(train_dataset)} training examples\")\nprint(f\"✓ Loaded {len(test_dataset)} test examples\")\n\n\nprint(\"\\nExample question:\")\nprint(train_dataset[0][\"question\"])\nprint(\"\\nExample answer:\")\nprint(train_dataset[0][\"answer\"])\nprint(\"\\nExtracted reasoning:\")\nprint(extract_reasoning(train_dataset[0][\"answer\"]))\nprint(\"\\nExtracted numerical answer:\")\nprint(extract_hash_answer(train_dataset[0][\"answer\"]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T02:53:58.675685Z","iopub.execute_input":"2025-12-21T02:53:58.676016Z","iopub.status.idle":"2025-12-21T02:53:59.578129Z","shell.execute_reply.started":"2025-12-21T02:53:58.676000Z","shell.execute_reply":"2025-12-21T02:53:59.577308Z"}},"outputs":[{"name":"stdout","text":"Loading GSM8K dataset...\n✓ Loaded 7473 training examples\n✓ Loaded 1319 test examples\n\nExample question:\nNatalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n\nExample answer:\nNatalia sold 48/2 = <<48/2=24>>24 clips in May.\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n#### 72\n\nExtracted reasoning:\nNatalia sold 48/2 = <<48/2=24>>24 clips in May.\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n\nExtracted numerical answer:\n72\n","output_type":"stream"}],"execution_count":49},{"cell_type":"markdown","source":"## Cell 13: Load GSM8K and format into strict SFT training examples\n\n- Loads GSM8K train/test splits via `datasets.load_dataset`.\n- Defines `clean_gsm8k_content()` to remove/normalize GSM8K-specific artifacts like `<<10+5=15>>`.\n- Defines `format_gsm8k_example(ex)` to build a single training string in Gemma chat format:\n  - user turn: system prompt + question\n  - model turn: `<reasoning> cleaned reasoning </reasoning>` + `<answer> extracted answer </answer>`\n\nProduces:\n- `formatted_train`: list of dicts with `{\"text\": ...}`\n- `formatted_test`: same for evaluation\n\n**Why this works**\nSFT teaches the model to imitate the “ideal completion” for the exact prompt you will use at inference time.\n","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nimport re\n\nreasoning_start = \"<reasoning>\"\nreasoning_end = \"</reasoning>\"\nsolution_start = \"<answer>\"\nsolution_end = \"</answer>\"\n\n\n# 1. Define the Cleaning Helper\ndef clean_gsm8k_content(text):\n    \"\"\"\n    Removes GSM8K specific calculation annotations.\n    Converts '<<10+5=15>>' to '(10+5=15)' or just removes them if preferred.\n    For SFT, replacing with parentheses is usually safer than deleting.\n    \"\"\"\n    if text is None:\n        return \"\"\n    # Replace << and >> with parentheses to make it standard math text\n    cleaned = text.replace(\"<<\", \"(\").replace(\">>\", \")\")\n    return cleaned\n\n# 2. Define the Formatter\ndef format_gsm8k_example(example):\n    \"\"\"\n    Formats training data with strict system instructions and data cleaning.\n    \"\"\"\n    question = example[\"question\"]\n    raw_answer = example[\"answer\"]\n    \n    # Extract parts\n    reasoning = extract_reasoning(raw_answer)\n    answer = extract_hash_answer(raw_answer)\n    \n    # --- APPLY CLEANING HERE ---\n    # We clean the reasoning part because that's where the <<...>> artifacts live.\n    #reasoning = clean_gsm8k_content(reasoning)\n    \n    # --- PROMPT CONSTRUCTION ---\n    \n    # 1. User Turn (Includes the strict instructions)\n    text = f\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n\\n{question}<end_of_turn>\\n\"\n    \n    # 2. Model Turn (The expected strict output)\n    text += f\"<start_of_turn>model\\n\"\n    text += f\"<reasoning>\\n{reasoning}\\n</reasoning>\\n\"\n    text += f\"<answer>\\n{answer}\\n</answer>\"\n    text += f\"<end_of_turn>\"\n\n    return {\"text\": text}\n\nprint(\"Refining dataset with CLEANING and STRICT System Prompt...\")\nformatted_train = [format_gsm8k_example(ex) for ex in train_dataset]\nformatted_test = [format_gsm8k_example(ex) for ex in test_dataset]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T02:54:05.791303Z","iopub.execute_input":"2025-12-21T02:54:05.791594Z","iopub.status.idle":"2025-12-21T02:54:05.994332Z","shell.execute_reply.started":"2025-12-21T02:54:05.791549Z","shell.execute_reply":"2025-12-21T02:54:05.993369Z"}},"outputs":[{"name":"stdout","text":"Refining dataset with CLEANING and STRICT System Prompt...\n","output_type":"stream"}],"execution_count":50},{"cell_type":"markdown","source":"## Cell 14: Print a sample formatted example\n\nPrints one formatted training example so you can validate:\n- the chat markers (`<start_of_turn>...`)\n- the system prompt presence\n- reasoning and answer tags\n- absence of GSM8K artifacts\n\nThis is a critical sanity check before launching a TPU training run.\n","metadata":{}},{"cell_type":"code","source":"print(\"-\" * 60)\nprint(formatted_train[100][\"text\"])\nprint(\"-\" * 60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T02:54:16.667583Z","iopub.execute_input":"2025-12-21T02:54:16.667847Z","iopub.status.idle":"2025-12-21T02:54:16.671860Z","shell.execute_reply.started":"2025-12-21T02:54:16.667826Z","shell.execute_reply":"2025-12-21T02:54:16.670821Z"}},"outputs":[{"name":"stdout","text":"------------------------------------------------------------\n<start_of_turn>user\nSolve the math problem. You must STRICTLY follow this format:\n1. Enclose your step-by-step logic inside <reasoning>...</reasoning> tags.\n2. Enclose the final numerical result inside <answer>...</answer> tags.\n\nA craft store makes a third of its sales in the fabric section, a quarter of its sales in the jewelry section, and the rest in the stationery section. They made 36 sales today. How many sales were in the stationery section?<end_of_turn>\n<start_of_turn>model\n<reasoning>\nThe craft store made 36 / 3 = (36/3=12)12 sales in the fabric section.\nIt made 36 / 4 = (36/4=9)9 sales in the jewelry section.\nThus, there were 36 - 12 - 9 = (36-12-9=15)15 sales in the stationery section.\n</reasoning>\n<answer>\n15\n</answer><end_of_turn>\n------------------------------------------------------------\n","output_type":"stream"}],"execution_count":51},{"cell_type":"markdown","source":"## Cell 15: Tokenization + Grain input pipelines (train/eval)\n\n- Defines `tokenize_function(example)` which:\n  - tokenizes the full supervised text\n  - separately tokenizes the **prompt prefix** up to `<start_of_turn>model\\n`\n  - builds masks so loss is applied primarily to the model completion portion (common SFT practice)\n  - pads/truncates to `MAX_SEQ_LENGTH`\n  - wraps everything into Tunix `TrainingInput`\n- Builds Grain datasets:\n  - shuffle + repeat for training\n  - batch for train and eval\n\n**Why Grain**\nGrain is optimized for JAX input pipelines and plays well with TPU training.\n\n**Note**\nThis cell contains `...` placeholders in the notebook source. Ensure the mask/padding logic is complete and executable before running.\n","metadata":{}},{"cell_type":"code","source":"import grain.python as grain\nimport numpy as np\nfrom tunix.sft.peft_trainer import TrainingInput\n\ndef tokenize_function(example):\n    full_text = example[\"text\"]\n    full_tokens = tokenizer.encode(full_text)\n    \n    \n    prompt_text = full_text.split(\"<start_of_turn>model\")[0] + \"<start_of_turn>model\\n\"\n    prompt_tokens = tokenizer.encode(prompt_text)\n    prompt_len = len(prompt_tokens)\n\n    # Padding/Truncation Logic\n    if len(full_tokens) > MAX_SEQ_LENGTH:\n        full_tokens = full_tokens[:MAX_SEQ_LENGTH]\n    else:\n        pad_token = tokenizer.pad_id() if hasattr(tokenizer, 'pad_id') else tokenizer.eos_id()\n        full_tokens = full_tokens + [pad_token] * (MAX_SEQ_LENGTH - len(full_tokens))\n\n    input_tokens = np.array(full_tokens, dtype=np.int32)\n    \n    # Create Mask\n    loss_mask = np.zeros_like(input_tokens, dtype=np.float32)\n    \n    # Enable loss only for the response part (ignoring padding)\n    seq_len = min(len(tokenizer.encode(full_text)), MAX_SEQ_LENGTH)\n    if seq_len > prompt_len:\n        loss_mask[prompt_len:seq_len] = 1.0\n\n    return TrainingInput(input_tokens=input_tokens, input_mask=loss_mask)\n\n\n# Create Grain datasets\ntrain_grain = (\n    grain.MapDataset.source(formatted_train)\n    .map(tokenize_function)\n    .shuffle(seed=42)\n    .repeat(NUM_EPOCHS)\n    .batch(batch_size=TRAIN_MICRO_BATCH_SIZE, drop_remainder=True)\n)\n\neval_grain = (\n    grain.MapDataset.source(formatted_test)\n    .map(tokenize_function)\n    .batch(batch_size=TRAIN_MICRO_BATCH_SIZE, drop_remainder=True)\n)\n\nprint(f\"✓ Train batches: {len(train_grain):,}\")\nprint(f\"✓ Eval batches: {len(eval_grain):,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T02:54:21.348800Z","iopub.execute_input":"2025-12-21T02:54:21.349078Z","iopub.status.idle":"2025-12-21T02:54:21.355955Z","shell.execute_reply.started":"2025-12-21T02:54:21.349060Z","shell.execute_reply":"2025-12-21T02:54:21.355068Z"}},"outputs":[{"name":"stdout","text":"✓ Train batches: 37,365\n✓ Eval batches: 659\n","output_type":"stream"}],"execution_count":52},{"cell_type":"markdown","source":"## Cell 16: Learning-rate schedule and optimizer (Optax)\n\n- Builds a warmup + cosine decay LR schedule.\n- Creates an optimizer chain:\n  1) global norm clipping (stability)\n  2) Adam moments\n  3) weight decay (regularization)\n  4) scheduled LR scaling\n  5) negative scale to perform gradient descent\n\nPrints the final optimizer settings for auditability.\n","metadata":{}},{"cell_type":"code","source":"import optax\n\nschedule = optax.warmup_cosine_decay_schedule(\n    init_value=0.0,\n    peak_value=LEARNING_RATE,\n    warmup_steps=WARMUP_STEPS,\n    decay_steps=MAX_STEPS - WARMUP_STEPS,\n    end_value=LEARNING_RATE * 0.1,\n)\n\n# Create optimizer chain\noptimizer = optax.chain(\n    optax.clip_by_global_norm(MAX_GRAD_NORM),\n    optax.scale_by_adam(\n        b1=ADAM_BETA1,\n        b2=ADAM_BETA2,\n        eps=ADAM_EPSILON,\n    ),\n    optax.add_decayed_weights(WEIGHT_DECAY),\n    optax.scale_by_schedule(schedule),\n    optax.scale(-1.0),  # Gradient descent\n)\n\nprint(\"✓ Optimizer configured:\")\nprint(f\"  Learning rate: {LEARNING_RATE}\")\nprint(f\"  Warmup steps: {WARMUP_STEPS}\")\nprint(f\"  Total steps: {MAX_STEPS}\")\nprint(f\"  Weight decay: {WEIGHT_DECAY}\")\nprint(f\"  Max grad norm: {MAX_GRAD_NORM}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T02:54:25.506938Z","iopub.execute_input":"2025-12-21T02:54:25.507199Z","iopub.status.idle":"2025-12-21T02:54:25.512072Z","shell.execute_reply.started":"2025-12-21T02:54:25.507182Z","shell.execute_reply":"2025-12-21T02:54:25.511228Z"}},"outputs":[{"name":"stdout","text":"✓ Optimizer configured:\n  Learning rate: 2e-05\n  Warmup steps: 50\n  Total steps: 3000\n  Weight decay: 0.01\n  Max grad norm: 1.0\n","output_type":"stream"}],"execution_count":53},{"cell_type":"markdown","source":"# training part starts","metadata":{}},{"cell_type":"markdown","source":"## Cell 17: Trainer configuration (checkpoints, logging, model input fn)\n\n- Configures Orbax checkpoint manager:\n  - save cadence\n  - retention policy (`max_to_keep`)\n- Builds a `TrainingConfig`:\n  - total steps, eval cadence, gradient accumulation, checkpoint/log directories\n  - TensorBoard metric logging\n- Defines `gen_model_input_fn(training_input)`:\n  - builds `positions` and causal `attention_mask` from non-padding tokens\n  - returns the dict expected by the Gemma model forward pass\n- Instantiates `PeftTrainer` and attaches the input adapter.\n\n**Important nuance**\nDespite the class name `PeftTrainer`, this notebook’s print statements suggest full fine-tuning. Confirm whether PEFT adapters are actually enabled; otherwise this is “full-parameter” training.\n","metadata":{}},{"cell_type":"code","source":"from tunix import PeftTrainer, TrainingConfig, MetricsLoggerOptions\nimport orbax.checkpoint as ocp\n\ncheckpointing_options = ocp.CheckpointManagerOptions(\n    save_interval_steps=SAVE_INTERVAL_STEPS,\n    max_to_keep=3,  # Keep last 3 checkpoints\n)\n\ntraining_config = TrainingConfig(\n    max_steps=MAX_STEPS,\n    eval_every_n_steps=EVAL_INTERVAL_STEPS,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    checkpoint_root_directory=CHECKPOINT_DIR,\n    checkpointing_options=checkpointing_options,\n    metrics_logging_options=MetricsLoggerOptions(\n        log_dir=TENSORBOARD_DIR,\n        flush_every_n_steps=LOG_INTERVAL_STEPS\n    ),\n)\n\nprint(\"✓ Training configuration created\")\nprint(f\"  Max steps: {MAX_STEPS}\")\nprint(f\"  Micro batch size: {TRAIN_MICRO_BATCH_SIZE}\")\nprint(f\"  Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"  Effective batch size: {TRAIN_MICRO_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"  Eval interval: {EVAL_INTERVAL_STEPS}\")\nprint(f\"  Save interval: {SAVE_INTERVAL_STEPS}\")\n\n# Model input function\nfrom tunix.sft import utils\n\ndef gen_model_input_fn(training_input):\n    \"\"\"Convert TrainingInput to model-compatible format.\"\"\"\n    pad_mask = training_input.input_tokens != 0\n    positions = utils.build_positions_from_mask(pad_mask)\n    attention_mask = utils.make_causal_attn_mask(pad_mask)\n    \n    return {\n        'input_tokens': training_input.input_tokens,\n        'input_mask': training_input.input_mask,\n        'positions': positions,\n        'attention_mask': attention_mask,\n    }\n\n\ntrainer = PeftTrainer(\n    model=gemma3_model,\n    optimizer=optimizer,\n    training_config=training_config,\n)\ntrainer = trainer.with_gen_model_input_fn(gen_model_input_fn)\n\nprint(\"✓ Trainer ready for training\")\nprint(f\"  Model: Gemma 3 1B (Full Fine-Tuning)\")\nprint(f\"  Max steps: {MAX_STEPS}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T02:54:30.456780Z","iopub.execute_input":"2025-12-21T02:54:30.457055Z","iopub.status.idle":"2025-12-21T02:54:32.685419Z","shell.execute_reply.started":"2025-12-21T02:54:30.457035Z","shell.execute_reply":"2025-12-21T02:54:32.684170Z"}},"outputs":[{"name":"stdout","text":"✓ Training configuration created\n  Max steps: 3000\n  Micro batch size: 2\n  Gradient accumulation: 4\n  Effective batch size: 8\n  Eval interval: 50\n  Save interval: 100\n✓ Trainer ready for training\n  Model: Gemma 3 1B (Full Fine-Tuning)\n  Max steps: 3000\n","output_type":"stream"}],"execution_count":54},{"cell_type":"markdown","source":"## Cell 18: Launch training and verify TPU usage via timing\n\n- Prints run metadata (steps, dataset sizes, batch and accumulation).\n- Re-checks parameter device placement (TPU vs CPU).\n- Calls `trainer.train(train_ds=..., eval_ds=...)`.\n- Reports total time, time per step, and checkpoint location.\n- Includes a heuristic “TPU vs CPU” check based on average step time after compilation.\n\n**Caveat**\nThe timing heuristic is rough; the first few steps include XLA compilation. For a more reliable check, confirm device placement and look at TPU utilization in the runtime.\n","metadata":{}},{"cell_type":"code","source":"### training the models\n\nimport time\n\nprint(\"=\"*60)\nprint(\"Starting Full Fine-Tuning on TPU v5e-8\")\nprint(\"=\"*60)\nprint(f\"Max steps: {MAX_STEPS}\")\nprint(f\"Training examples: {len(formatted_train)}\")\nprint(f\"Eval examples: {len(formatted_test)}\")\nprint(f\"Batch size: {TRAIN_MICRO_BATCH_SIZE}\")\nprint(f\"Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\nprint(\"=\"*60)\n\n\nall_params = nnx.state(gemma3_model)\nparam_leaves = jax.tree_util.tree_leaves(all_params)\nif len(param_leaves) > 0:\n    sample_param = param_leaves[0]\n    if hasattr(sample_param, 'devices'):\n        devices = sample_param.devices()\n        if len(devices) > 0:\n            device_kind = list(devices)[0].device_kind\n            print(f\"✓ Model parameters are on: {device_kind}\")\n            if 'tpu' not in device_kind.lower():\n                print(f\"⚠️  WARNING: Model params on {device_kind}, not TPU!\")\n                print(f\"⚠️  Training will run on CPU and produce wrong results!\")\n            else:\n                print(f\"✓✓✓ CONFIRMED: Model is ready for TPU training!\")\n        else:\n            print(\"⚠️  No devices found for model parameters\")\n    else:\n        print(\"⚠️  Cannot check device placement\")\nelse:\n    print(\"⚠️  No model parameters found\")\nprint(\"=\"*60)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"IMPORTANT: First training step will take 2-5 minutes\")\nprint(\"=\"*60)\nprint(\"JAX is compiling all functions (happens on CPU).\")\nprint(\"After first step completes, TPU will be used and steps will be MUCH faster.\")\nprint(\"You should see 'Compiling...' messages initially.\")\nprint(\"=\"*60)\n\nprint(\"\\nStarting training...\")\nstart_time = time.time()\n\n\ntrainer.train(\n    train_ds=train_grain,\n    eval_ds=eval_grain,\n)\n\nend_time = time.time()\ntotal_time = end_time - start_time\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Training Completed!\")\nprint(\"=\"*60)\nprint(f\"Total training time: {total_time:.1f} seconds ({total_time/60:.1f} minutes)\")\nprint(f\"Average time per step: {total_time/MAX_STEPS:.1f} seconds\")\nprint(f\"Checkpoints saved to: {CHECKPOINT_DIR}\")\nprint(\"=\"*60)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"POST-TRAINING: Verify TPU was used\")\nprint(\"=\"*60)\nprint(f\"Expected TPU time: 5-15 seconds per step after compilation\")\nprint(f\"Your average: {total_time/MAX_STEPS:.1f} seconds per step\")\nif total_time/MAX_STEPS < 1.0:\n    print(\"❌ WARNING: Training ran on CPU, not TPU!\")\n    print(\"Results will be incorrect. Check that model is properly sharded.\")\nelse:\n    print(\"✓ Training timing looks correct for TPU usage!\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T02:54:41.856788Z","iopub.execute_input":"2025-12-21T02:54:41.857072Z","iopub.status.idle":"2025-12-21T02:56:51.229215Z","shell.execute_reply.started":"2025-12-21T02:54:41.857051Z","shell.execute_reply":"2025-12-21T02:56:51.227930Z"}},"outputs":[{"name":"stdout","text":"============================================================\nStarting Full Fine-Tuning on TPU v5e-8\n============================================================\nMax steps: 3000\nTraining examples: 7473\nEval examples: 1319\nBatch size: 2\nGradient accumulation: 4\n============================================================\n✓ Model parameters are on: TPU v5 lite\n✓✓✓ CONFIRMED: Model is ready for TPU training!\n============================================================\n\n============================================================\nIMPORTANT: First training step will take 2-5 minutes\n============================================================\nJAX is compiling all functions (happens on CPU).\nAfter first step completes, TPU will be used and steps will be MUCH faster.\nYou should see 'Compiling...' messages initially.\n============================================================\n\nStarting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: 100%|##########| 3000/3000 [00:00<?, ?step/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1829d38427c4b129ff7476e5b66aee8"}},"metadata":{}},{"name":"stdout","text":"\n============================================================\nTraining Completed!\n============================================================\nTotal training time: 129.0 seconds (2.1 minutes)\nAverage time per step: 0.0 seconds\nCheckpoints saved to: /kaggle/working/outputs_sft_full/checkpoints\n============================================================\n\n============================================================\nPOST-TRAINING: Verify TPU was used\n============================================================\nExpected TPU time: 5-15 seconds per step after compilation\nYour average: 0.0 seconds per step\n❌ WARNING: Training ran on CPU, not TPU!\nResults will be incorrect. Check that model is properly sharded.\n============================================================\n","output_type":"stream"}],"execution_count":55},{"cell_type":"markdown","source":"## Cell 19: Rebuild the generation sampler for the fine-tuned model\n\nAfter training, you typically re-instantiate:\n- cache config\n- `Sampler`\n\nThis ensures generation uses the updated in-memory weights and a fresh cache, then you can re-run evaluation with the same parsing/scoring code.\n","metadata":{}},{"cell_type":"code","source":"from tunix.generate import sampler as sampler_lib\nimport json\nimport os\n\n\ncache_config = sampler_lib.CacheConfig(\n    cache_size=MAX_SEQ_LENGTH + 512,\n    num_layers=model_config.num_layers,\n    num_kv_heads=model_config.num_kv_heads,\n    head_dim=model_config.head_dim,\n)\n\n\ngeneration_sampler = sampler_lib.Sampler(\n    transformer=gemma3_model,\n    tokenizer=tokenizer,\n    cache_config=cache_config,\n)\n\n\ndef generate_inference_prompt(question):\n    # Match the training exactly: Same System Prompt, No One-Shot needed anymore.\n    text = f\"<start_of_turn>user\\n{SYSTEM_PROMPT}\\n\\n{question}<end_of_turn>\\n\"\n    text += f\"<start_of_turn>model\\n<reasoning>\\n\" \n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T02:47:01.199038Z","iopub.execute_input":"2025-12-21T02:47:01.199370Z","iopub.status.idle":"2025-12-21T02:47:01.237982Z","shell.execute_reply.started":"2025-12-21T02:47:01.199345Z","shell.execute_reply":"2025-12-21T02:47:01.236912Z"}},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"# Eval","metadata":{}},{"cell_type":"markdown","source":"## Cell 20: Define test questions for quick smoke testing\n\nCreates a small list of arithmetic/word problems to validate:\n- the model follows the `<reasoning>` and `<answer>` format\n- the fine-tuned model improved on the types of questions you care about\n","metadata":{}},{"cell_type":"code","source":"# Test questions\ntest_questions = [\n    \"What is the square root of 144?\",\n    \"If a shirt costs $25 and is on sale for 20% off, what is the sale price?\",\n    \"A train travels 60 miles in 45 minutes. What is its speed in miles per hour?\",\n    \"What is 15% of 200?\",\n]\n\nprint(\"=\"*60)\nprint(\"Testing Trained Model (Strict Format)\")\nprint(\"=\"*60)\n\nfor i, question in enumerate(test_questions, 1):\n    # 1. Generate the formatted prompt\n    prompt = generate_inference_prompt(question)\n\n    print(f\"\\n[Test {i}] Question: {question}\")\n    print(\"-\" * 60)\n\n    # 2. Run Generation\n    sampler_output = generation_sampler(\n        input_strings=[prompt],\n        max_generation_steps=512,\n        temperature=0.01,  # Near-greedy for math\n        top_k=1,\n    )\n\n    # 3. Extract and Clean Response\n    response = sampler_output.text[0]\n    \n    # Manual Stop: Cut off text if the model generates <end_of_turn>\n    # This fixes the looping issue seen in Test 4\n    if \"<end_of_turn>\" in response:\n        response = response.split(\"<end_of_turn>\")[0]\n\n    print(f\"Response:\\n{response}\")\n    print(\"=\" * 60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T02:47:05.832027Z","iopub.execute_input":"2025-12-21T02:47:05.832274Z","iopub.status.idle":"2025-12-21T02:47:21.983015Z","shell.execute_reply.started":"2025-12-21T02:47:05.832255Z","shell.execute_reply":"2025-12-21T02:47:21.981904Z"}},"outputs":[{"name":"stdout","text":"============================================================\nTesting Trained Model (Strict Format)\n============================================================\n\n[Test 1] Question: What is the square root of 144?\n------------------------------------------------------------\nResponse:\n</reasoning>\nThe square root of 144 is 12 because 12*12=(12*12=144)144\n</reasoning>\n<answer>\n12\n</answer>\n============================================================\n\n[Test 2] Question: If a shirt costs $25 and is on sale for 20% off, what is the sale price?\n------------------------------------------------------------\nResponse:\nreasoning>\nThe discount for the shirt is $25 x 20/100 = $(25*20/100=5)5.\nSo the shirt is sold for $25 - $5 = $(25-5=20)20.\n</reasoning>\n<answer>\n20\n</answer>\n============================================================\n\n[Test 3] Question: A train travels 60 miles in 45 minutes. What is its speed in miles per hour?\n------------------------------------------------------------\nResponse:\n45 minutes is 45/60 = (45/60=0.75)0.75 hours.\nSo the speed is 60/0.75 = (60/0.75=80)80 mph.\n</reasoning>\n<answer>\n80\n</answer>\n============================================================\n\n[Test 4] Question: What is 15% of 200?\n------------------------------------------------------------\nResponse:\nreasoning>\n15% of 200 is (15/100)*200 = (15*200/100=30)30.\n</reasoning>\n<answer>\n30\n</answer>\n============================================================\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"# Test questions\ntest_questions = [\n    \"The present age of A is twice that of B. After 10 years, A will be 1.5 times B. What is A's present age?\",\n    \"If a shirt costs $25 and is on sale for 20% off, what is the sale price?\",\n    \"A train travels 60 miles in 45 minutes. What is its speed in miles per hour?\",\n    \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\",\n]\n\nprint(\"=\"*60)\nprint(\"Testing Trained Model (Strict Format)\")\nprint(\"=\"*60)\n\nfor i, question in enumerate(test_questions, 1):\n    # 1. Generate the formatted prompt\n    prompt = generate_inference_prompt(question)\n\n    print(f\"\\n[Test {i}] Question: {question}\")\n    print(\"-\" * 60)\n\n    # 2. Run Generation\n    sampler_output = generation_sampler(\n        input_strings=[prompt],\n        max_generation_steps=512,\n        temperature=0.01,  # Near-greedy for math\n        top_k=1,\n    )\n\n    # 3. Extract and Clean Response\n    response = sampler_output.text[0]\n    \n    # Manual Stop: Cut off text if the model generates <end_of_turn>\n    # This fixes the looping issue seen in Test 4\n    if \"<end_of_turn>\" in response:\n        response = response.split(\"<end_of_turn>\")[0]\n\n    print(f\"Response:\\n{response}\")\n    print(\"=\" * 60)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T02:47:22.856859Z","iopub.execute_input":"2025-12-21T02:47:22.857057Z","iopub.status.idle":"2025-12-21T02:47:23.601939Z","shell.execute_reply.started":"2025-12-21T02:47:22.857041Z","shell.execute_reply":"2025-12-21T02:47:23.600898Z"}},"outputs":[{"name":"stdout","text":"============================================================\nTesting Trained Model (Strict Format)\n============================================================\n\n[Test 1] Question: The present age of A is twice that of B. After 10 years, A will be 1.5 times B. What is A's present age?\n------------------------------------------------------------\nResponse:\n\n============================================================\n\n[Test 2] Question: If a shirt costs $25 and is on sale for 20% off, what is the sale price?\n------------------------------------------------------------\nResponse:\nreasoning>\nThe discount for the shirt is $25 x 20/100 = $(25*20/100=5)5.\nSo the shirt is sold for $25 - $5 = $(25-5=20)20.\n</reasoning>\n<answer>\n20\n</answer>\n============================================================\n\n[Test 3] Question: A train travels 60 miles in 45 minutes. What is its speed in miles per hour?\n------------------------------------------------------------\nResponse:\n45 minutes is 45/60 = (45/60=0.75)0.75 hours.\nSo the speed is 60/0.75 = (60/0.75=80)80 mph.\n</reasoning>\n<answer>\n80\n</answer>\n============================================================\n\n[Test 4] Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n------------------------------------------------------------\nResponse:\nNatalia sold 48/2 = (48/2=24)24 clips in May.\nNatalia sold 48+24 = (48+24=72)72 clips altogether in April and May.\n</reasoning>\n<answer>\n72\n</answer>\n============================================================\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T03:02:52.751264Z","iopub.execute_input":"2025-12-21T03:02:52.751602Z","iopub.status.idle":"2025-12-21T03:02:52.894959Z","shell.execute_reply.started":"2025-12-21T03:02:52.751576Z","shell.execute_reply":"2025-12-21T03:02:52.893727Z"}},"outputs":[],"execution_count":62},{"cell_type":"markdown","source":"## Cell 21: Evaluation loop with (optional) self-consistency voting\n\nImplements a more robust evaluation strategy:\n\n- Generate `VOTE_SAMPLES` completions per question (with `TEMPERATURE > 0`).\n- Parse each completion into a final answer candidate.\n- Use majority vote (`collections.Counter`) to pick the most frequent answer.\n- Compute final accuracy and log failures with candidate distributions.\n\n**Why self-consistency helps**\nMany math problems have multiple valid reasoning paths; sampling increases the chance you get at least one correct path, and voting reduces variance.\n\n**Tradeoff**\nHigher `VOTE_SAMPLES` improves accuracy but increases inference cost linearly.\n","metadata":{}},{"cell_type":"code","source":"import collections\nimport time\nimport re\nfrom tqdm.auto import tqdm\n\n\nVOTE_SAMPLES = 1 \n\n# Temperature must be > 0 to get diverse reasoning paths\n# 0.6 is standard for Self-Consistency\nTEMPERATURE = 0.7 \n\n# Max tokens for the answer\nMAX_GEN_STEPS = 512\n\nprint(\"\\n\" + \"=\"*60)\nprint(f\"Evaluating with Majority Voting (k={VOTE_SAMPLES})\")\nprint(\"=\"*60)\n\n\ndef normalize_answer(answer_str):\n    \"\"\"Normalize answer string for comparison.\"\"\"\n    if answer_str is None:\n        return None\n    s = str(answer_str).strip().lower()\n    s = s.replace('$', '').replace(',', '').replace('£', '').replace('€', '')\n    if s.endswith('.'):\n        s = s[:-1]\n    return s\n\ndef extract_answer_robust(response):\n    \"\"\"\n    Extracts answers using a cascade of patterns (XML -> Boxed -> Text).\n    \"\"\"\n    # 1. Try <answer> tags\n    xml_match = re.search(r\"<answer>\\s*(.*?)\\s*</answer>\", response, re.DOTALL)\n    if xml_match:\n        return xml_match.group(1)\n\n    # 2. Try LaTeX \\boxed{}\n    boxed_match = re.search(r\"\\\\boxed\\{([^}]+)\\}\", response)\n    if boxed_match:\n        return boxed_match.group(1)\n\n    # 3. Try \"Final Answer\" text patterns\n    text_match = re.search(r\"(?:final answer|answer is)[:\\s]*([0-9\\.]+)\", response, re.IGNORECASE)\n    if text_match:\n        return text_match.group(1)\n\n    # 4. Fallback: Last number\n    numbers = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", response)\n    if numbers:\n        return numbers[-1]\n    return None\n\ndef get_majority_vote(candidates):\n    \"\"\"Returns the most common answer from a list of candidates.\"\"\"\n    # Filter out None values\n    valid_candidates = [c for c in candidates if c is not None]\n    \n    if not valid_candidates:\n        return None\n    \n    # Count frequency\n    counter = collections.Counter(valid_candidates)\n    \n    # Get the most common element ((value, count) tuple)\n    most_common, count = counter.most_common(1)[0]\n    return most_common\n\n\n# Load dataset if not already loaded\nif 'test_dataset' not in globals():\n    from datasets import load_dataset\n    test_dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n\ntotal_examples = len(test_dataset)\ncorrect_count = 0\nstart_time = time.time()\n\n# Store failures for analysis\nfailures = []\n\nfor idx in tqdm(range(total_examples), desc=\"Voting\"):\n    example = test_dataset[idx]\n    question = example[\"question\"]\n    \n    # Get Ground Truth\n    ground_truth_raw = extract_hash_answer(example[\"answer\"])\n    ground_truth_norm = normalize_answer(ground_truth_raw)\n\n    # Prepare Prompt\n    prompt = generate_inference_prompt(question)\n    \n    # Create Batch: Replicate the prompt VOTE_SAMPLES times\n    # This sends 8 identical prompts to the model at once\n    batch_prompts = [prompt] * VOTE_SAMPLES\n\n    try:\n        # Generate samples in parallel\n        sampler_output = generation_sampler(\n            input_strings=batch_prompts,\n            max_generation_steps=MAX_GEN_STEPS,\n            temperature=TEMPERATURE,\n            top_k=40, # Allow diversity for voting\n        )\n        \n        # Extract answers from all samples\n        candidates = []\n        for response_text in sampler_output.text:\n            # Cleanup stop tokens\n            if \"<end_of_turn>\" in response_text:\n                response_text = response_text.split(\"<end_of_turn>\")[0]\n            \n            # Extract\n            raw_ans = extract_answer_robust(response_text)\n            norm_ans = normalize_answer(raw_ans)\n            candidates.append(norm_ans)\n            \n        # Perform Majority Vote\n        final_prediction = get_majority_vote(candidates)\n        \n        # Check Correctness\n        is_correct = False\n        if final_prediction is not None and ground_truth_norm is not None:\n            try:\n                is_correct = float(final_prediction) == float(ground_truth_norm)\n            except ValueError:\n                is_correct = final_prediction == ground_truth_norm\n        \n        if is_correct:\n            correct_count += 1\n        else:\n            # Log failure for inspection\n            failures.append({\n                \"q\": question,\n                \"gt\": ground_truth_norm,\n                \"pred\": final_prediction,\n                \"candidates\": candidates\n            })\n\n    except Exception as e:\n        print(f\"Error on example {idx}: {e}\")\n\nend_time = time.time()\ntotal_time = end_time - start_time\n\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"MAJORITY VOTING RESULTS\")\nprint(\"=\"*60)\nprint(f\"Total Time: {total_time:.1f}s ({total_time/total_examples:.2f}s per question)\")\nprint(f\"Samples per Question: {VOTE_SAMPLES}\")\nprint(\"-\" * 60)\nprint(f\"Final Accuracy: {correct_count}/{total_examples} ({100*correct_count/total_examples:.2f}%)\")\nprint(\"=\"*60)\n\n# Show a sample failure to see voting behavior\nif failures:\n    print(\"\\nSample Failure (Voting Analysis):\")\n    f = failures[0]\n    print(f\"Question: {f['q'][:100]}...\")\n    print(f\"Ground Truth: {f['gt']}\")\n    print(f\"Voted Prediction: {f['pred']}\")\n    print(f\"Vote Distribution: {f['candidates']}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import HfApi\nfrom huggingface_hub import login, upload_folder\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\napi = HfApi()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\nlogin(token=hf_token)\n\nupload_folder(\n    folder_path=\"outputs_sft_full\",\n    repo_id=\"liuxiaohua72/lxh_gemma3\",\n    repo_type=\"model\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Thank you\n\nStill, a lot of training is required on this model. This was a basic training strategy","metadata":{}}]}